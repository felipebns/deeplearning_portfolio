{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#template-de-entrega","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"exercicio1/main/","title":"Exercicio 1","text":""},{"location":"exercicio1/main/#grupo","title":"Grupo","text":"<ol> <li>Felipe Bakowski Nantes de Souza</li> </ol>"},{"location":"exercicio1/main/#entrega","title":"Entrega","text":"<ul> <li> Roteiro 1 - Data 05/09/2025</li> </ul>"},{"location":"exercicio1/main/#objetivo","title":"Objetivo","text":"<p>O objetivo desse exerc\u00edcio \u00e9 aprender a gerar dados sint\u00e9ticos e como fazer um pr\u00e9-processamento adequado</p>"},{"location":"exercicio1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Dados sint\u00e9ticos de 4 classes gerados e j\u00e1 separados por retas</p> <p></p> <p>C\u00f3digo</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnum_samples = 100\n\n# ---------------------------- CLASS 0 -----------------------------\n\nmean_class_0 = [2, 3] # [mean_x, mean_y]\nstd_class_0 = [0.8, 2.5] # [std x, std y]\n\nclass_0_points_x = np.random.normal(loc=mean_class_0[0], scale=std_class_0[0], size=num_samples)\nclass_0_points_y = np.random.normal(loc=mean_class_0[1], scale=std_class_0[1], size=num_samples)\n\n# ---------------------------- CLASS 1 -----------------------------\n\nmean_class_1 = [5, 6] # [mean_x, mean_y]\nstd_class_1 = [1.2, 1.9] # [std x, std y]\n\nclass_1_points_x = np.random.normal(loc=mean_class_1[0], scale=std_class_1[0], size=num_samples)\nclass_1_points_y = np.random.normal(loc=mean_class_1[1], scale=std_class_1[1], size=num_samples)\n\n# ---------------------------- CLASS 2 -----------------------------\n\nmean_class_2 = [8, 1] # [mean_x, mean_y]\nstd_class_2 = [0.9, 0.9] # [std x, std y]\n\nclass_2_points_x = np.random.normal(loc=mean_class_2[0], scale=std_class_2[0], size=num_samples)\nclass_2_points_y = np.random.normal(loc=mean_class_2[1], scale=std_class_2[1], size=num_samples)\n\n# ---------------------------- CLASS 3 -----------------------------\n\nmean_class_3 = [15, 4] # [mean_x, mean_y]\nstd_class_3 = [0.5, 2.0] # [std x, std y]\n\nclass_3_points_x = np.random.normal(loc=mean_class_3[0], scale=std_class_3[0], size=num_samples)\nclass_3_points_y = np.random.normal(loc=mean_class_3[1], scale=std_class_3[1], size=num_samples)\n\n\n# ---------------------------- PLOT -----------------------------\n# ---------------------------- ---- -----------------------------\n\nplt.scatter(class_0_points_x, class_0_points_y, c='green', label='Class 0')\nplt.scatter(class_1_points_x, class_1_points_y, c='red', label='Class 1')\nplt.scatter(class_2_points_x, class_2_points_y, c='blue', label='Class 2')\nplt.scatter(class_3_points_x, class_3_points_y, c='black', label='Class 3')\n\nplt.legend()\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Multiple Data Series Scatter Plot')\nplt.show()\n</code></pre> <p>An\u00e1lise</p> <p>Esse gr\u00e1fico mostra 4 classes com caracter\u00edsticas distintas, o fato que mais ilustra isso \u00e9 a discrep\u00e2ncia da vari\u00e2ncia entre a classe 3 e a classe 1. Enquanto esta \u00e9 bem centrada e f\u00e1cil de fazer uma linha que separe ela das outras, o mesmo n\u00e3o pode ser dito para aquela, j\u00e1 que ela tem grande vari\u00e2ncia e acaba por ter grande \"overlap\" com as outras classes tamb\u00e9m, fazendo sua separa\u00e7\u00e3o mais dif\u00edcil.</p> <p>Apesar de uma separa\u00e7\u00e3o linear funcinar at\u00e9 que bem para este cen\u00e1rio, em casos mais complexos ela n\u00e3o ser\u00e1 mais suficiente. Esse fato fica evidente quando eu tentei aproximar uma separa\u00e7\u00e3o linear que uma rede neural faria, apesar da classe 0, classe 2 e classe 3 terem espa\u00e7os bem delimitados e com uma vari\u00e2ncia baixa, a classe 1 se mostrou um desafio maior para tal feito.</p>"},{"location":"exercicio1/main/#tarefa-2","title":"Tarefa 2","text":"<p>Dados sint\u00e9ticos de 5 features j\u00e1 reduzidos a 2 com PCA</p> <p></p> <p>C\u00f3digo</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA as pca\nfrom sklearn.preprocessing import StandardScaler\n\n# ---------------------------- CLASS A -----------------------------\n\n# Define the mean vector\nmean_a = [0, 0, 0, 0, 0]\n\n# Define the covariance matrix\ncov_a = [[1, 0.8, 0.1, 0, 0], [0.8, 1, 0.3, 0, 0], [0.1, 0.3, 1, 0.5, 0], [0, 0, 0.5, 1, 0.2], [0, 0, 0, 0.2, 1]] \n\n# Draw 500 samples from the multivariate normal distribution\nclass_a = np.random.multivariate_normal(mean_a, cov_a, size=500)\n\n# ---------------------------- CLASS B -----------------------------\n\n# Define the mean vector\nmean_b = [1.5, 1.5, 1.5, 1.5, 1.5]\n\n# Define the covariance matrix\ncov_b = [[1.5, -0.7, 0.2, 0, 0], [-0.7, 1.5, 0.4, 0, 0], [0.2, 0.4, 1.5, 0.6, 0], [0, 0, 0.6, 1.5, 0.3], [0, 0, 0, 0.3, 1.5]] \n\n# Draw 500 samples from the multivariate normal distribution\nclass_b = np.random.multivariate_normal(mean_b, cov_b, size=500)\n\n# ---------------------------- PCA -----------------------------\n\n# Junta os dados\nall_data = np.vstack([class_a, class_b])\nsklearn_pca = pca(n_components=2)\nall_pca = sklearn_pca.fit_transform(all_data)\n\n# Separa de volta\nclass_a_pca = all_pca[:len(class_a)]\nclass_b_pca = all_pca[len(class_a):]\n\n# ---------------------------- PLOT -----------------------------\n\n\nplt.scatter(class_a_pca[:,0], class_a_pca[:,1], c='blue', label='Class A')\nplt.scatter(class_b_pca[:,0], class_b_pca[:,1], c='red', label='Class B')\n\nplt.legend()\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Data after PCA')\nplt.show()\n</code></pre> <p>An\u00e1lise</p> <p>A priori, pode-se observar que existe grande overlap entre as 2 classes ap\u00f3s aplica\u00e7\u00e3o do m\u00e9todo PCA e que ambas apresentam grande vari\u00e2ncia.</p> <p>Sendo assim, pode-se afirmar que essas classes N\u00c3O s\u00e3o linearmente separ\u00e1veis, isso deve ao fato de que uma reta n\u00e3o \u00e9 suficiente para seperar elas com precis\u00e3o e precisaria de um modelo mais complexo para fazer essa classifica\u00e7\u00e3o. Ainda, partindo da ideia de que a grosso modo \"uma reta \u00e9 um neur\u00f4nio\" fica evidente que precisamos de um modelo de rede neural de m\u00faltiplas camadas para realizar essa fun\u00e7\u00e3o</p>"},{"location":"exercicio1/main/#tarefa-3","title":"Tarefa 3","text":"<p>C\u00f3digo</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv(\"train.csv\")\n\n# print(df.columns[df.isnull().any()]) #encontra colunas com valores faltando\n# print(df)\n\n#Configura\u00e7\u00e3o para arrumar fillna deprecated\npd.set_option('future.no_silent_downcasting', True)\n\n#Data cleaning\n#Categorial\ndf.dropna(subset=['Name'], inplace=True)  \ndf['HomePlanet'] = df['HomePlanet'].fillna(df['HomePlanet'].mode()[0])\ndf['CryoSleep'] = df['CryoSleep'].fillna(df['CryoSleep'].mode()[0])\ndf['Cabin'] = df['Cabin'].fillna(df['Cabin'].mode()[0])\ndf['Destination'] = df['Destination'].fillna(df['Destination'].mode()[0])\ndf['VIP'] = df['VIP'].fillna(df['VIP'].mode()[0])\n\n# Num\u00e9ricas\ndf['Age'] = df['Age'].fillna(df['Age'].median())\ndf['RoomService'] = df['RoomService'].fillna(df['RoomService'].median())\ndf['FoodCourt'] = df['FoodCourt'].fillna(df['FoodCourt'].median())\ndf['ShoppingMall'] = df['ShoppingMall'].fillna(df['ShoppingMall'].median())\ndf['Spa'] = df['Spa'].fillna(df['Spa'].median())\ndf['VRDeck'] = df['VRDeck'].fillna(df['VRDeck'].median())\n\nage_prev = df['Age']\nfood_prev = df['FoodCourt']\n\n# Convert categorical variables\nlabel_encoder = LabelEncoder()\ndf['HomePlanet'] = label_encoder.fit_transform(df['HomePlanet'])\ndf['CryoSleep'] = label_encoder.fit_transform(df['CryoSleep'])\ndf['Cabin'] = label_encoder.fit_transform(df['Cabin'])\ndf['Destination'] = label_encoder.fit_transform(df['Destination'])\ndf['Name'] = label_encoder.fit_transform(df['Name'])\n#True or false\ndf['VIP'] = label_encoder.fit_transform(df['VIP'])\ndf['Transported'] = label_encoder.fit_transform(df['Transported'])\n\n# print(df.columns[df.isnull().any()]) #roda mais uma vez para verificar se tem null ainda\n\n# print(30*'-')\n# print(df)\n\n#Normalizing Data\ndf['Age'] = df['Age'].apply(lambda x: (x-df['Age'].mean())/df['Age'].std())\ndf['RoomService'] = df['RoomService'].apply(\n    lambda x: 2 * (x - df['RoomService'].min()) / (df['RoomService'].max() - df['RoomService'].min()) - 1\n)\ndf['FoodCourt'] = df['FoodCourt'].apply(\n    lambda x: 2 * (x - df['FoodCourt'].min()) / (df['FoodCourt'].max() - df['FoodCourt'].min()) - 1\n)\ndf['ShoppingMall'] = df['ShoppingMall'].apply(\n    lambda x: 2 * (x - df['ShoppingMall'].min()) / (df['ShoppingMall'].max() - df['ShoppingMall'].min()) - 1\n)\ndf['Spa'] = df['Spa'].apply(\n    lambda x: 2 * (x - df['Spa'].min()) / (df['Spa'].max() - df['Spa'].min()) - 1\n)\ndf['VRDeck'] = df['VRDeck'].apply(\n    lambda x: 2 * (x - df['VRDeck'].min()) / (df['VRDeck'].max() - df['VRDeck'].min()) - 1\n)\n\n# print(30*'-')\nprint(df)\n\n#Histogramas\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\naxes[0, 0].hist(age_prev, bins=30)\naxes[0, 0].set_title(\"Age - Antes da Normaliza\u00e7\u00e3o\")\naxes[0, 0].set_xlabel(\"Idade\")\naxes[0, 0].set_ylabel(\"Frequ\u00eancia\")\n\naxes[0, 1].hist(df['Age'], bins=30)\naxes[0, 1].set_title(\"Age - Depois (Z-Score)\")\naxes[0, 1].set_xlabel(\"Z-score\")\n\naxes[1, 0].hist(food_prev, bins=30)\naxes[1, 0].set_title(\"FoodCourt - Antes da Normaliza\u00e7\u00e3o\")\naxes[1, 0].set_xlabel(\"Valor gasto\")\n\naxes[1, 1].hist(df['FoodCourt'], bins=30)\naxes[1, 1].set_title(\"FoodCourt - Depois ([-1, 1])\")\naxes[1, 1].set_xlabel(\"Valor Normalizado\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>An\u00e1lise</p> <p>De in\u00edcio, podemos analisar quais features s\u00e3o categoricas e quais s\u00e3o num\u00e9ricas, as categoricas s\u00e3o: \"HomePlanet\", \"CryoSleep\", \"Cabin\", \"Destination\", \"Name\", \"VIP\", \"Transported\". Enquanto as num\u00e9ricas s\u00e3o: \"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"ShoppingMall\", \"Spa\", \"VRDeck\".</p> <p>Algumas dessas vari\u00e1veis s\u00e3o bem auto explicativas, como idade, mas agora irei explicar algumas das menos intuitivas: A vari\u00e1vel CryoSleep mostra se o passageiro optou por permanecer em anima\u00e7\u00e3o suspensa (sono criog\u00eanico) durante a viagem, o que significa que ele ficava confinado em sua cabine, a vari\u00e1vel Transported informa se o passageiro foi transportado para outra dimens\u00e3o durante o incidente da nave, sendo esta a vari\u00e1vel-alvo que deve ser prevista nos modelos. As vari\u00e1veis RoomService, FoodCourt, ShoppingMall, Spa e VRDeck registram, respectivamente, os valores gastos em diferentes servi\u00e7os do navio: servi\u00e7o de quarto, pra\u00e7a de alimenta\u00e7\u00e3o, lojas, spa e experi\u00eancias de realidade virtual no deck de VR. Essas vari\u00e1veis num\u00e9ricas permitem avaliar tanto o perfil de consumo dos passageiros quanto poss\u00edveis padr\u00f5es de comportamento.</p> <p>Mas, nem todos esses dados est\u00e3o perfeitos, a verdade \u00e9 que quase todas as colunas tem valores faltando (print(df.columns[df.isnull().any()]) mostra quais). Agora, como podemos lidar com isso ? o mais f\u00e1cil foi eliminar todas as linhas em que o nome do passageira estava faltando, pois n\u00e3o h\u00e1 nenhum m\u00e9todo que justifique inventar um nome para a pessoa de modo que fa\u00e7a sentido. Em rela\u00e7\u00e3o a todas as outras vari\u00e1veis categ\u00f3ricas podemos preench\u00ea-las com a moda dessa coluna, isso minimiza os danos a distor\u00e7\u00e3o dos dados e nos permite reaproveitar o resto dessa linha. Nesse passo, com as vari\u00e1veis quantitivas preenchemos os valores faltando com a mediana (preferimos em rela\u00e7\u00e3o a m\u00e9dia pois causa uma distor\u00e7\u00e3o menor nesse cen\u00e1rio em que h\u00e1 muitos valores extremos, veremos isso depois), com a mesma justificativa da anterior.</p> <p>Para encoding das vari\u00e1veis categorias foi usado o m\u00e9todo de one-hot encoding pela sua facilidade e ent\u00e3o aplicamos a Standardization na vari\u00e1vel age, pois ela geralmente j\u00e1 segue uma distribui\u00e7\u00e3o normal e uma Normaliza\u00e7\u00e3o em todas as outras num\u00e9ricas, pois elas n\u00e3o seguem esse distribui\u00e7\u00e3o, s\u00e3o constrituidas de muitos valores extremos.</p> <p>Assim, pode-se ver a diferen\u00e7a entras os dados sem esse tratamento e depois desse tratamento na figura acima para 2 features</p>"},{"location":"exercicio2/main/","title":"Exercicio 2","text":""},{"location":"exercicio2/main/#grupo","title":"Grupo","text":"<ol> <li>Felipe Bakowski Nantes de Souza</li> </ol>"},{"location":"exercicio2/main/#entrega","title":"Entrega","text":"<ul> <li> Roteiro 2 - Data 14/09/2025</li> </ul>"},{"location":"exercicio2/main/#objetivo","title":"Objetivo","text":"<p>Aprender a implementar um perceptron e entender suas limita\u00e7\u00f5es para separa\u00e7\u00e3o de classes</p>"},{"location":"exercicio2/main/#tarefa-1","title":"Tarefa 1","text":"<p>Gere 2 classes, com 1000 pontos cada, em um espa\u00e7o 2D e utilize suas implentan\u00e7\u00e3o de perceptron para separ\u00e1-las</p>"},{"location":"exercicio2/main/#especificacoes","title":"Especifica\u00e7\u00f5es:","text":"<pre><code>Class 0:\n    - M\u00e9dia [1.5, 1.5]\n    - Cov matrix: [[0.5,0], [0,0.5]]\nClass 1:\n    - M\u00e9dia [5,5]\n    - Cov matrix: [[0.5,0], [0,0.5]]\n</code></pre>"},{"location":"exercicio2/main/#pontos-do-primeiro-caso-com-reta-definida-pelo-perceptron","title":"Pontos do primeiro caso com reta definida pelo perceptron","text":""},{"location":"exercicio2/main/#convergencia-por-epochs","title":"Converg\u00eancia por epochs","text":""},{"location":"exercicio2/main/#parametros-escolhidos-pelo-perceptron","title":"Par\u00e2metros escolhidos pelo perceptron:","text":"<pre><code>Novos weights: [0.03266194 0.01985417] e novo bias: -0.15\n</code></pre> <p>C\u00f3digo</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport perceptron as pt\n\nnum_samples = 1000\n\nmean_class_0 = [1.5, 1.5]\nstd_class_0 = [0.5, 0.5] \n\nclass_0_points_x1 = np.random.normal(loc=mean_class_0[0], scale=std_class_0[0], size=num_samples)\nclass_0_points_x2 = np.random.normal(loc=mean_class_0[1], scale=std_class_0[1], size=num_samples)\nclass_0_points_y = pd.Series([0]*num_samples)\n\nmean_class_1 = [5, 5] \nstd_class_1 = [0.5, 0.5] \n\nclass_1_points_x1 = np.random.normal(loc=mean_class_1[0], scale=std_class_1[0], size=num_samples)\nclass_1_points_x2 = np.random.normal(loc=mean_class_1[1], scale=std_class_1[1], size=num_samples)\nclass_1_points_y = pd.Series([1]*num_samples)\n\nc0_x1 = pd.Series(class_0_points_x1)\nc1_x1 = pd.Series(class_1_points_x1)\n\nc0_x2 = pd.Series(class_0_points_x2)\nc1_x2 = pd.Series(class_1_points_x2)\n\nx1 = pd.concat([c0_x1, c1_x1], ignore_index=True)\nx2 = pd.concat([c0_x2, c1_x2], ignore_index=True)\ny = pd.concat([class_0_points_y, class_1_points_y], ignore_index=True)\n\ndf = pd.DataFrame({\n    'x1':x1,\n    'x2':x2,\n    'y':y\n})\n\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nw = [0,0]\nb = 0\neta = 0.01\nepochs = 100\nacertos = []\n\nw,b,acertos = pt.train(df, w=w, b=b, eta=eta, epochs=epochs)\n\nprint(f\"Novos wights: {w} e novo bias: {b}\")\n\nepochs_list = np.arange(0,len(acertos),1)\n\nx_reta, y_reta = pt.create_line(w,b)\nacertos = pt.calculate_accuracy(acertos, len(df)) \n\nplt.scatter(epochs_list, acertos)\nplt.xlabel('Epochs')\nplt.ylabel('% Acertos')\nplt.title('Acertos por epochs')\nplt.show()\n\nplt.scatter(class_0_points_x1, class_0_points_x2, c='blue', label='Class 0')\nplt.scatter(class_1_points_x1, class_1_points_x2, c='red', label='Class 1')\nplt.plot(x_reta, y_reta, color='black')\n\nplt.legend()\nplt.xlabel('X1-axis')\nplt.ylabel('X2-axis')\nplt.title('Multiple Data Series Scatter Plot')\nplt.show()\n</code></pre> <p>An\u00e1lise</p> <p>Pode-se observar que nessa caso \u00e9 poss\u00edvel realizar uma separa\u00e7\u00e3o dessas 2 classes com apenas 1 perceptron, isso se deve ao fato de ambas tem m\u00e9dias muito distantes uma da outra e um desvio padr\u00e3o muito baixo nos 2 casos. Assim, esse \u00e9 um caso bem fact\u00edvel com um perceptron.</p>"},{"location":"exercicio2/main/#tarefa-2","title":"Tarefa 2","text":"<p>Gere 2 classes, com 1000 pontos cada, em um espa\u00e7o 2D e utilize suas implentan\u00e7\u00e3o de perceptron para separ\u00e1-las</p>"},{"location":"exercicio2/main/#especificacoes_1","title":"Especifica\u00e7\u00f5es:","text":"<pre><code>Class 0:\n    - M\u00e9dia [3, 3]\n    - Cov matrix: [[1.5,0], [0,1.5]]\nClass 1:\n    - M\u00e9dia [4,4]\n    - Cov matrix: [[1.5,0], [0,1.5]]\n</code></pre>"},{"location":"exercicio2/main/#pontos-do-segundo-caso-com-reta-definida-pelo-perceptron","title":"Pontos do segundo caso com reta definida pelo perceptron","text":""},{"location":"exercicio2/main/#convergencia-por-epochs_1","title":"Converg\u00eancia por epochs","text":""},{"location":"exercicio2/main/#parametros-escolhidos-pelo-perceptron_1","title":"Par\u00e2metros escolhidos pelo perceptron:","text":"<pre><code>Novos weights: [0.05104332 0.0504264 ] e novo bias: -0.25000000000000006\n</code></pre> <p>C\u00f3digo</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport perceptron as pt\n\nnum_samples = 1000\n\nmean_class_0 = [3, 3]\nstd_class_0 = [1.5, 1.5] \n\nclass_0_points_x1 = np.random.normal(loc=mean_class_0[0], scale=std_class_0[0], size=num_samples)\nclass_0_points_x2 = np.random.normal(loc=mean_class_0[1], scale=std_class_0[1], size=num_samples)\nclass_0_points_y = pd.Series([0]*num_samples)\n\nmean_class_1 = [4, 4] \nstd_class_1 = [1.5, 1.5] \n\nclass_1_points_x1 = np.random.normal(loc=mean_class_1[0], scale=std_class_1[0], size=num_samples)\nclass_1_points_x2 = np.random.normal(loc=mean_class_1[1], scale=std_class_1[1], size=num_samples)\nclass_1_points_y = pd.Series([1]*num_samples)\n\nc0_x1 = pd.Series(class_0_points_x1)\nc1_x1 = pd.Series(class_1_points_x1)\n\nc0_x2 = pd.Series(class_0_points_x2)\nc1_x2 = pd.Series(class_1_points_x2)\n\nx1 = pd.concat([c0_x1, c1_x1], ignore_index=True)\nx2 = pd.concat([c0_x2, c1_x2], ignore_index=True)\ny = pd.concat([class_0_points_y, class_1_points_y], ignore_index=True)\n\ndf = pd.DataFrame({\n    'x1':x1,\n    'x2':x2,\n    'y':y\n})\n\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nw = [0,0]\nb = 0\neta = 0.01\nepochs = 100\nacertos = []\n\nw,b,acertos = pt.train(df, w=w, b=b, eta=eta, epochs=epochs)\n\nprint(f\"Novos wights: {w} e novo bias: {b}\")\n\nepochs_list = np.arange(0,len(acertos),1)\n\nx_reta, y_reta = pt.create_line(w,b)\nacertos = pt.calculate_accuracy(acertos, len(df)) \n\nplt.scatter(epochs_list, acertos)\nplt.xlabel('Epochs')\nplt.ylabel('% Acertos')\nplt.title('Acertos por epochs')\nplt.show()\n\nplt.scatter(class_0_points_x1, class_0_points_x2, c='blue', label='Class 0')\nplt.scatter(class_1_points_x1, class_1_points_x2, c='red', label='Class 1')\nplt.plot(x_reta, y_reta, color='black')\n\nplt.legend()\nplt.xlabel('X1-axis')\nplt.ylabel('X2-axis')\nplt.title('Multiple Data Series Scatter Plot')\nplt.show()\n</code></pre> <p>An\u00e1lise</p> <p>Nesse cen\u00e1rio, observa-se que \u00e9 imposs\u00edvel 1 perceptron dar conta de separar essas 2 classes, a primeira evid\u00eancia disso \u00e9 a aus\u00eancia de converg\u00eancia do modelo, ele fica alteranando entre 50% e 60% de precis\u00e3o (classifica\u00e7\u00f5es corretas / Total samples). Depois, podemos observar que existe grande overlap entre as classes, devido as m\u00e9dias parecidas e grande desvio padr\u00e3o delas. Desse modo, fica n\u00edtido que a reta n\u00e3o consegue separ\u00e1-las propriamente.</p>"},{"location":"exercicio2/main/#implementacao-perceptron","title":"Implementa\u00e7\u00e3o perceptron","text":"<pre><code>import pandas as pd\nimport numpy as np\n\ndef calculate_z(w,x,b):\n    return np.dot(w,x) + b\n\ndef activation(z):\n    if z &gt;= 0:\n        return 1\n    else:\n        return 0\n\ndef update(erro, w, b, eta, x):\n    return w + eta*erro*x , b + eta*erro\n\ndef train(df, w, b, eta, epochs):\n    x1 = df['x1']\n    x2 = df['x2']\n    y = df['y']\n    features = [x1,x2]\n    x = []\n    w_train = w\n    b_train = b\n    epoch = 0\n    acertos = 0\n    acertos_por_epoch = []\n\n    while epoch &lt; epochs: #um epoch eh um pass completo sobre os dados\n\n        for i in range(len(y)):\n            for feature in features:\n                x.append(float(feature[i]))\n\n            z = calculate_z(w_train,x,b_train)\n            y_calculated = activation(z)\n            erro = y[i] - y_calculated\n\n            if erro != 0:\n                w_train, b_train = update(erro, w_train, b_train, eta, np.array(x))\n            else:\n                acertos += 1\n\n            if acertos == len(y):\n                acertos_por_epoch.append(acertos)\n                return w_train, b_train, acertos_por_epoch\n\n            x = []\n\n        acertos_por_epoch.append(acertos)\n        acertos = 0\n        epoch += 1\n\n    return w_train, b_train, acertos_por_epoch\n\ndef calculate_accuracy(acertos, len_dataset):\n    acertos = pd.Series(acertos)\n    acertos = (acertos/len_dataset)*100\n    return acertos\n\ndef create_line(w,b):\n    x_reta = np.arange(0,8,0.01)\n    y = []\n    for x in x_reta:\n        r = -x*(w[0]/w[1]) - (b/w[1])\n        y.append(r)\n    return x_reta, y\n</code></pre>"},{"location":"exercicio3/main/","title":"Exercicio 3","text":""},{"location":"exercicio3/main/#grupo","title":"Grupo","text":"<ol> <li>Felipe Bakowski Nantes de Souza</li> </ol>"},{"location":"exercicio3/main/#entrega","title":"Entrega","text":"<ul> <li> Roteiro 3 - Data 21/09/2025</li> </ul>"},{"location":"exercicio3/main/#objetivo","title":"Objetivo","text":"<p>Implementar um MLP e test\u00e1-lo em casos distintos. Ainda, validar o conhecimento em calcular forward propagation, back, loss, etc, manualmente</p>"},{"location":"exercicio3/main/#tarefa-1","title":"Tarefa 1","text":"<p>Para os seguintes par\u00e2metros:</p> <pre><code>x = [0.5, -0.2]\ny = 1\n\nw1 = [[0.3, -0.1],\n    [0.2,  0.4]]\n\nb1 = [0.1, -0.2]\n\nw2 = [0.5, -0.3]\n\nb2 = 0.2\n\neta = 0.3\n\nact func = tanh\n</code></pre> <p>Calcule o forward pass, loss, backward pass e parameter update.</p> <p>Para tal utilizamos o seguinte c\u00f3digo:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\n# Manual calculation of MLP\n\n# Preparation\nx = np.array([0.5, -0.2])\ny = 1\nw_hidden = np.array([[0.3, -0.1],\n                     [0.2, 0.4]])\nb_hidden = np.array([0.1, -0.2])\nw_out = np.array([0.5, -0.3])\nb_out = 0.2\neta = 0.1\n\n# === Forward pass ===\nz_hidden = np.dot(w_hidden, x) + b_hidden\na_hidden = np.tanh(z_hidden)\nz_out = np.dot(w_out, a_hidden) + b_out\ny_out = np.tanh(z_out)\n\nloss = (y - y_out)**2\n\nprint(\"\\n=== FORWARD PASS ===\")\nprint(f\"Input x: {x}\")\nprint(f\"Hidden pre-activation z_hidden: {z_hidden}\")\nprint(f\"Hidden activation a_hidden (tanh): {a_hidden}\")\nprint(f\"Output pre-activation z_out: {z_out:.4f}\")\nprint(f\"Output y_out (tanh): {y_out:.4f}\")\nprint(f\"Loss: {loss:.6f}\")\n\n# === Backward Pass ===\ndloss_dy_out = -2*(y - y_out)\ndloss_dz_out = dloss_dy_out*(1 - y_out**2)\n\ndloss_dw_out1 = dloss_dz_out*a_hidden[0]\ndloss_dw_out2 = dloss_dz_out*a_hidden[1]\ndloss_db_out = dloss_dz_out\n\ndloss_da_hidden1 = dloss_dz_out*w_out[0]\ndloss_da_hidden2 = dloss_dz_out*w_out[1]\ndloss_dz_hidden1 = dloss_da_hidden1 * (1 - a_hidden[0]**2)\ndloss_dz_hidden2 = dloss_da_hidden2 * (1 - a_hidden[1]**2)\n\ndloss_dw_hidden11 = dloss_dz_hidden1*x[0]\ndloss_dw_hidden21 = dloss_dz_hidden1*x[1]\ndloss_dw_hidden12 = dloss_dz_hidden2*x[0]\ndloss_dw_hidden22 = dloss_dz_hidden2*x[1]\ndloss_db_hidden1 = dloss_dz_hidden1\ndloss_db_hidden2 = dloss_dz_hidden2\n\n# === PRINT BACKPROP RESULTS ===\nprint(\"\\n=== BACKWARD PASS ===\")\nprint(f\"dL/dy_out: {dloss_dy_out:.6f}\")\nprint(f\"dL/dz_out: {dloss_dz_out:.6f}\")\n\nprint(\"\\n-- Gradientes sa\u00edda --\")\nprint(f\"dL/dw_out: [{dloss_dw_out1:.6f}, {dloss_dw_out2:.6f}]\")\nprint(f\"dL/db_out: {dloss_db_out:.6f}\")\n\nprint(\"\\n-- Propaga\u00e7\u00e3o para hidden --\")\nprint(f\"dL/da_hidden: [{dloss_da_hidden1:.6f}, {dloss_da_hidden2:.6f}]\")\nprint(f\"dL/dz_hidden: [{dloss_dz_hidden1:.6f}, {dloss_dz_hidden2:.6f}]\")\n\nprint(\"\\n-- Gradientes hidden weights --\")\ngrad_hidden = pd.DataFrame(\n    [[dloss_dw_hidden11, dloss_dw_hidden21],\n     [dloss_dw_hidden12, dloss_dw_hidden22]],\n    columns=[\"x1\", \"x2\"],\n    index=[\"h1\", \"h2\"]\n)\nprint(grad_hidden)\n\nprint(\"\\n-- Gradientes hidden biases --\")\nprint(f\"dL/db_hidden: [{dloss_db_hidden1:.6f}, {dloss_db_hidden2:.6f}]\")\n\n# ==== PARAMETER UPDATES =====\nprint(\"\\n=== UPDATE PARAMETERS ===\")\nprint(\"Antes da atualiza\u00e7\u00e3o:\")\nprint(\"w_hidden:\\n\", w_hidden)\nprint(\"b_hidden:\", b_hidden)\nprint(\"w_out:\", w_out)\nprint(\"b_out:\", b_out)\n\n# Atualiza\u00e7\u00e3o da camada escondida\nw_hidden[0][0] -= eta * dloss_dw_hidden11\nw_hidden[0][1] -= eta * dloss_dw_hidden21\nw_hidden[1][0] -= eta * dloss_dw_hidden12\nw_hidden[1][1] -= eta * dloss_dw_hidden22\n\nb_hidden[0] -= eta * dloss_db_hidden1\nb_hidden[1] -= eta * dloss_db_hidden2\n\n# Atualiza\u00e7\u00e3o da camada de sa\u00edda\nw_out[0] -= eta * dloss_dw_out1\nw_out[1] -= eta * dloss_dw_out2\nb_out    -= eta * dloss_db_out\n\nprint(\"\\nDepois da atualiza\u00e7\u00e3o:\")\nprint(\"w_hidden:\\n\", w_hidden)\nprint(\"b_hidden:\", b_hidden)\nprint(\"w_out:\", w_out)\nprint(\"b_out:\", b_out)\n</code></pre> <p>que gerou o seguinte output:</p> <pre><code>=== FORWARD PASS ===\nInput x: [ 0.5 -0.2]\nHidden pre-activation z_hidden: [ 0.27 -0.18]\nHidden activation a_hidden (tanh): [ 0.26362484 -0.17808087]\nOutput pre-activation z_out: 0.3852\nOutput y_out (tanh): 0.3672\nLoss: 0.400377\n\n=== BACKWARD PASS ===\ndL/dy_out: -1.265507\ndL/dz_out: -1.094828\n\n-- Gradientes sa\u00edda --\ndL/dw_out: [-0.288624, 0.194968]\ndL/db_out: -1.094828\n\n-- Propaga\u00e7\u00e3o para hidden --\ndL/da_hidden: [-0.547414, 0.328448]\ndL/dz_hidden: [-0.509370, 0.318032]\n\n-- Gradientes hidden weights --\n          x1        x2\nh1 -0.254685  0.101874\nh2  0.159016 -0.063606\n\n-- Gradientes hidden biases --\ndL/db_hidden: [-0.509370, 0.318032]\n\n=== UPDATE PARAMETERS ===\nAntes da atualiza\u00e7\u00e3o:\nw_hidden:\n [[ 0.3 -0.1]\n [ 0.2  0.4]]\nb_hidden: [ 0.1 -0.2]\nw_out: [ 0.5 -0.3]\nb_out: 0.2\n\nDepois da atualiza\u00e7\u00e3o:\nw_hidden:\n [[ 0.32546849 -0.1101874 ]\n [ 0.18409838  0.40636065]]\nb_hidden: [ 0.15093698 -0.23180324]\nw_out: [ 0.52886238 -0.31949679]\nb_out: 0.30948279147136\n</code></pre>"},{"location":"exercicio3/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"exercicio3/main/#nota-para-as-seguintes-tarefas-foi-utilizada-uma-mesma-mlp-que-da-conta-de-suprir-todas-as-necessidades-codigo-dela-no-final-da-pagina","title":"Nota: para as seguintes tarefas foi utilizada uma mesma MLP que da conta de suprir todas as necessidades, c\u00f3digo dela no final da p\u00e1gina","text":"<p>Utilizando as seguintes especifica\u00e7\u00f5es: </p> <p>N amostras: 1000</p> <p>N classes: 2</p> <p>N de clusters por classe:  classe 0 -&gt; 1 cluster classe 1 -&gt; 2 clusters</p> <p>crie dados que seguem essas caracter\u00edsticas e mande os resultado.</p> <p>Assim, o c\u00f3digo para criar esses dados \u00e9 o seguinte:</p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom mlp import mlp\n\n# ===== BINARY CLASSIFICATION (Exercise 2) =====\n# Classe 0: 1 cluster\nX0, y0 = make_classification(\n    n_samples=500, n_classes=1, n_clusters_per_class=1,\n    n_features=2, n_informative=2, n_redundant=0,\n    class_sep=2.0, random_state=42\n)\ny0[:] = 0  # for\u00e7a classe 0\n\n# Classe 1: 2 clusters\nX1, y1 = make_classification(\n    n_samples=500, n_classes=1, n_clusters_per_class=2,\n    n_features=2, n_informative=2, n_redundant=0,\n    class_sep=2.0, random_state=24\n)\ny1[:] = 1  # for\u00e7a classe 1\n\n# Combina\nX = np.vstack([X0, X1])\ny = np.hstack([y0, y1])\n\n# Split train/test (80/20)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Cria MLP para classifica\u00e7\u00e3o bin\u00e1ria\nmlp = mlp(\n    n_features=2,\n    n_hidden_layers=1,\n    n_neurons_per_layer=[4, 1],  # hidden=4, output=1\n    activation='tanh',\n    loss='mse', #saida \u00e9 bin\u00e1ria, utilizamos mse\n    optimizer='gd',\n    epochs=200,\n    eta=0.1\n)\n\n# Treina\nmlp.train(X_train, y_train)\n\n# Testa e avalia\ny_pred = mlp.test(X_test)\nmlp.evaluate(X_test, y_test, plot_confusion=True, plot_roc=True, preds=y_pred)\n</code></pre> <p>e o modelo resultante \u00e9 o seguinte:</p> <pre><code>=== Inicializa\u00e7\u00e3o de Pesos e Biases ===\nCamada 1:\nW1 shape (4, 2):\n[[ 0.06282189 -0.01995681]\n [-0.09948727 -0.08671114]\n [ 0.08370109 -0.10908174]\n [-0.01757068  0.09519239]]\nb1 shape (4,):\n[0. 0. 0. 0.]\n\nCamada 2:\nW2 shape (1, 4):\n[[ 0.0665476  -0.14808445  0.11787561 -0.08045228]]\nb2 shape (1,):\n[0.]\n\nEpoch 0, Loss: 0.200536\nEpoch 10, Loss: 0.137519\nEpoch 20, Loss: 0.135654\nEpoch 30, Loss: 0.135681\nTreinamento encerrado no epoch 35 (converg\u00eancia detectada).\nAccuracy: 89.00%\n\n=== Pesos e Biases do Modelo ===\n\nCamada 1:\n  Pesos W1 (shape (4, 2)):\n[[ 0.01028162 -0.00596175]\n [-2.2332656  -2.97995227]\n [-1.69882465 -3.34039644]\n [-0.02222276  0.02191908]]\n  Biases b1 (shape (4,)):\n[ 0.03770969  5.99630873 -1.43428069 -0.1020545 ]\n\nCamada 2:\n  Pesos W2 (shape (1, 4)):\n[[-0.00147906 -0.91664343  0.8594697   0.00384365]]\n  Biases b2 (shape (1,)):\n[1.97320113]\n</code></pre> <p>Ainda, observa-se as imagens do roc curve e da matriz de confus\u00e3o que deixam em evid\u00eancia outras caracter\u00edsticas do modelos</p> <p></p> <p></p>"},{"location":"exercicio3/main/#tarefa-3","title":"Tarefa 3","text":"<p>Muito parecido com a tarefa 2, utilizando as seguintes especifica\u00e7\u00f5es: </p> <p>N amostras: 1500</p> <p>N classes: 3</p> <p>N features: 4</p> <p>N de clusters por classe:  classe 0 -&gt; 2 cluster classe 1 -&gt; 3 clusters classe 2 -&gt; 4 clusters</p> <p>crie dados que seguem essas caracter\u00edsticas e mande os resultado.</p> <p>Assim, o c\u00f3digo para criar esses dados \u00e9 o seguinte:</p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom mlp import mlp\n\n# ===== MULTI-CLASS CLASSIFICATION (Exercise 3) =====\n# Classe 0: 2 clusters\nX0, y0 = make_classification(\n    n_samples=500, n_classes=1, n_clusters_per_class=2,\n    n_features=4, n_informative=4, n_redundant=0,\n    class_sep=2.0, random_state=42\n)\ny0[:] = 0\n\n# Classe 1: 3 clusters\nX1, y1 = make_classification(\n    n_samples=500, n_classes=1, n_clusters_per_class=3,\n    n_features=4, n_informative=4, n_redundant=0,\n    class_sep=2.0, random_state=24\n)\ny1[:] = 1\n\n# Classe 2: 4 clusters\nX2, y2 = make_classification(\n    n_samples=500, n_classes=1, n_clusters_per_class=4,\n    n_features=4, n_informative=4, n_redundant=0,\n    class_sep=2.0, random_state=100\n)\ny2[:] = 2\n\n# Combina\nX = np.vstack([X0, X1, X2])\ny = np.hstack([y0, y1, y2])\n\n# Split train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Cria MLP para multi-class\nmlp = mlp(\n    n_features=4,\n    n_hidden_layers=1,\n    n_neurons_per_layer=[8, 3],  # hidden=8, output=3 classes\n    activation='tanh',\n    loss='cross_entropy',\n    optimizer='gd',\n    epochs=300,\n    eta=0.05\n)\n\n# Treina\nmlp.train(X_train, y_train)\n\n# Testa e avalia\ny_pred = mlp.test(X_test)\nmlp.evaluate(X_test, y_test, plot_confusion=True, plot_roc=False, preds=y_pred)\n</code></pre> <p>e o resultado foi o segiunte:</p> <pre><code>=== Inicializa\u00e7\u00e3o de Pesos e Biases ===\nCamada 1:\nW1 shape (8, 4):\n[[-0.15012286  0.03636618  0.00755215  0.11669222]\n [-0.08320915  0.06254752 -0.06307116 -0.14782926]\n [ 0.0022151  -0.14332554 -0.05004609 -0.0201836 ]\n [-0.00959626 -0.08452745  0.06800672  0.04447651]\n [-0.06675591  0.02716564 -0.0402755   0.04110338]\n [ 0.01647392  0.23689506  0.13759678 -0.17216953]\n [ 0.13695956 -0.01969437 -0.02480733  0.06700953]\n [-0.1538137  -0.07199269 -0.23851082  0.08865847]]\nb1 shape (8,):\n[0. 0. 0. 0. 0. 0. 0. 0.]\n\nCamada 2:\nW2 shape (3, 8):\n[[-0.03102423  0.14114412  0.12651862  0.0525337   0.00188353  0.0778287\n   0.21716552 -0.10621928]\n [ 0.10797846 -0.00570225 -0.10403077  0.11305206 -0.0683529  -0.01938096\n  -0.03513866  0.06930014]\n [ 0.07062139  0.14148664 -0.03282687 -0.04611272 -0.2125965   0.14508146\n  -0.13255499 -0.07127306]]\nb2 shape (3,):\n[0. 0. 0.]\n\nEpoch 0, Loss: 0.426007\nEpoch 10, Loss: 0.199110\nEpoch 20, Loss: 0.153921\nEpoch 30, Loss: 0.160203\nEpoch 40, Loss: 0.164695\nEpoch 50, Loss: 0.169913\nEpoch 60, Loss: 0.173882\nEpoch 70, Loss: 0.171059\nEpoch 80, Loss: 0.146784\nEpoch 90, Loss: 0.157961\nEpoch 100, Loss: 0.136661\nEpoch 110, Loss: 0.140420\nEpoch 120, Loss: 0.142970\nEpoch 130, Loss: 0.161925\nEpoch 140, Loss: 0.174006\nEpoch 150, Loss: 0.124596\nEpoch 160, Loss: 0.127589\nEpoch 170, Loss: 0.133139\nEpoch 180, Loss: 0.136089\nEpoch 190, Loss: 0.127153\nEpoch 200, Loss: 0.152746\nEpoch 210, Loss: 0.140961\nEpoch 220, Loss: 0.146886\nEpoch 230, Loss: 0.117297\nEpoch 240, Loss: 0.125047\nEpoch 250, Loss: 0.118332\nEpoch 260, Loss: 0.133416\nEpoch 270, Loss: 0.157566\nEpoch 280, Loss: 0.115870\nEpoch 290, Loss: 0.141657\nAccuracy: 94.33%\n\n=== Pesos e Biases do Modelo ===\n\nCamada 1:\n  Pesos W1 (shape (8, 4)):\n[[ -0.08576299  -0.95087168   2.55816272  13.44105675]\n [ -6.19685875  -1.0703442    0.06318581  -4.74594758]\n [ -0.30086178  -5.80590108  -3.15007815  -3.05409815]\n [ 10.22280012  -1.71382584   1.54665281   3.83600643]\n [ -0.67082362   1.81448785  -7.83430673  -2.71605366]\n [  3.42071321   9.41193187  -5.31067459   8.37335265]\n [ -5.95157244   0.94694413   0.79513151 -10.44803795]\n [ -3.83236454   1.58829969  -6.61690407   4.25813743]]\n  Biases b1 (shape (8,)):\n[  2.98018584  -8.2687564  -17.15145087   3.00833864  -2.61779085\n -18.47840645   4.66698319  -1.45933255]\n\nCamada 2:\n  Pesos W2 (shape (3, 8)):\n[[-3.08806844 -0.40361496  2.68544436 -1.59783599  1.93742736  0.79406231\n   1.26835691  1.03757026]\n [ 1.12279544 -0.83468701 -0.32344479  1.70018782 -1.86252433  3.18272202\n   1.1324417   0.35434438]\n [ 2.11284863  1.51523047 -2.37233858  0.01712121 -0.35396892 -3.77325513\n  -2.35132674 -1.50010685]]\n  Biases b2 (shape (3,)):\n[-1.54010396  2.56898611 -1.02888214]\n</code></pre> <p>Ainda, observa-se a imagen da matriz de confus\u00e3o que deixa em evid\u00eancia outras caracter\u00edsticas do modelos</p> <p></p>"},{"location":"exercicio3/main/#tarefa-4","title":"Tarefa 4","text":"<p>Por fim, repetiremos a mesma coisa do exerc\u00edcio 3, mas com 2 camadas ocultas.</p> <p>O c\u00f3digo para tal foi: </p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom mlp import mlp\n# ===== MULTI-CLASS CLASSIFICATION (Exercise 4) =====\n# Classe 0: 2 clusters\nX0, y0 = make_classification(\n    n_samples=500, n_classes=1, n_clusters_per_class=2,\n    n_features=4, n_informative=4, n_redundant=0,\n    class_sep=2.0, random_state=42\n)\ny0[:] = 0\n\n# Classe 1: 3 clusters\nX1, y1 = make_classification(\n    n_samples=500, n_classes=1, n_clusters_per_class=3,\n    n_features=4, n_informative=4, n_redundant=0,\n    class_sep=2.0, random_state=24\n)\ny1[:] = 1\n\n# Classe 2: 4 clusters\nX2, y2 = make_classification(\n    n_samples=500, n_classes=1, n_clusters_per_class=4,\n    n_features=4, n_informative=4, n_redundant=0,\n    class_sep=2.0, random_state=100\n)\ny2[:] = 2\n\n# Combina\nX = np.vstack([X0, X1, X2])\ny = np.hstack([y0, y1, y2])\n\n# Split train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Cria MLP para multi-class\n# ===== MULTI-CLASS WITH DEEPER MLP (Exercise 4) =====\nmlp_deep = mlp(\n    n_features=4,\n    n_hidden_layers=2,\n    n_neurons_per_layer=[16, 8, 3],  # duas hidden layers\n    activation='relu', #relu melhora performance\n    loss='cross_entropy',\n    optimizer='gd',\n    epochs=400, #mais epochs por ser maior\n    eta=0.01\n)\n\nmlp_deep.train(X_train, y_train)\n\ny_pred = mlp_deep.test(X_test)\nmlp_deep.evaluate(X_test, y_test, plot_confusion=True, plot_roc=False, preds=y_pred)\n</code></pre> <p>e o resultado foi o seguinte:</p> <pre><code>=== Inicializa\u00e7\u00e3o de Pesos e Biases ===\nCamada 1:\nW1 shape (16, 4):\n[[ 0.00555123  0.02945733  0.00320483  0.14107248]\n [ 0.01309987 -0.14536763 -0.24914801 -0.1097325 ]\n [ 0.13263989 -0.00845282  0.04363311 -0.20942593]\n [ 0.06151256  0.17526377  0.06525152 -0.143594  ]\n [ 0.19053969  0.05164986 -0.06384301 -0.13528166]\n [-0.01802708 -0.06539238 -0.23653229 -0.06647837]\n [-0.1773437   0.18719514 -0.2432306  -0.13664237]\n [-0.03559021  0.09356135  0.00909633 -0.02496799]\n [-0.010703   -0.05643333  0.14452321 -0.12340063]\n [-0.11520311  0.12193497 -0.01601216 -0.04162409]\n [ 0.10112067 -0.06794277 -0.09689412 -0.07876592]\n [ 0.10129985  0.13837398 -0.0192343   0.28110855]\n [ 0.06734521  0.04200509  0.06857739 -0.0089593 ]\n [ 0.03230203 -0.086233    0.05157866 -0.08939185]\n [-0.05631417  0.09558788  0.03310292 -0.03783207]\n [ 0.26892547 -0.0355394  -0.23973207  0.12095571]]\nb1 shape (16,):\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\nCamada 2:\nW2 shape (8, 16):\n[[ 0.02722147  0.07925187 -0.08892502  0.07395878 -0.03866726  0.04836275\n   0.03062031 -0.01350536  0.00136201  0.0866263  -0.11018394 -0.00879913\n   0.04369193  0.08227613  0.0774328   0.06628226]\n [-0.00382254 -0.04122877 -0.12089201 -0.09174723  0.28529764  0.04758449\n  -0.12776212  0.00118261 -0.22349046  0.18041981 -0.01872715 -0.05201189\n   0.00592675  0.01669519  0.11267001  0.06858279]\n [ 0.0127937  -0.03001005  0.01187981 -0.0564881   0.07511489 -0.34500628\n  -0.07107095  0.11538678 -0.01051689  0.0867737   0.15186372  0.04087284\n   0.06905849  0.14079224 -0.18074581  0.06372337]\n [-0.04562775  0.01619738 -0.0616143   0.00100512  0.13642802 -0.06340487\n   0.13042471 -0.02625564 -0.06957138 -0.0269656  -0.08705771 -0.11467694\n  -0.02810464  0.08168548  0.03388796  0.13286287]\n [ 0.08442033  0.23117066 -0.06388346  0.04641671 -0.01899345  0.03415299\n  -0.15404318  0.01264817  0.08692396 -0.13046417 -0.07150701 -0.1247061\n   0.11768296  0.08659426 -0.07457327  0.02828019]\n [-0.00576693  0.01818947 -0.04820858 -0.07797701  0.00879961  0.04139584\n   0.0744317  -0.00071307 -0.0222676  -0.05408615  0.0778405   0.15156107\n  -0.17894417 -0.06650762 -0.14505942 -0.0273448 ]\n [ 0.13834086  0.06803163 -0.07831411 -0.11383326 -0.19915929  0.05239984\n  -0.06936135 -0.13220569 -0.09276729  0.10489427  0.01956362  0.08235377\n   0.16126182 -0.24129253 -0.1078609  -0.07034368]\n [-0.00679845  0.11362595 -0.0065863   0.0774349   0.12396009  0.14655074\n  -0.16693398 -0.21766298  0.08333175  0.11371362 -0.27184616 -0.19571742\n   0.05006519 -0.02464831  0.0496306   0.13814309]]\nb2 shape (8,):\n[0. 0. 0. 0. 0. 0. 0. 0.]\n\nCamada 3:\nW3 shape (3, 8):\n[[ 0.13737013 -0.00819777  0.05503834  0.02554672 -0.07860347 -0.16416347\n  -0.05241293 -0.12545059]\n [ 0.08305538 -0.02370211 -0.00149418  0.12830182 -0.07896425 -0.0304211\n   0.03274002  0.02992113]\n [ 0.14912918  0.04469714  0.01246757  0.01173774 -0.03409813 -0.12439819\n  -0.01339562  0.04655981]]\nb3 shape (3,):\n[0. 0. 0.]\n\nEpoch 0, Loss: 0.888729\nEpoch 10, Loss: 0.144946\nEpoch 20, Loss: 0.113786\nEpoch 30, Loss: 0.095721\nEpoch 40, Loss: 0.083704\nEpoch 50, Loss: 0.075874\nEpoch 60, Loss: 0.070778\nEpoch 70, Loss: 0.066706\nEpoch 80, Loss: 0.064859\nEpoch 90, Loss: 0.062447\nEpoch 100, Loss: 0.058853\nEpoch 110, Loss: 0.060413\nEpoch 120, Loss: 0.058912\nEpoch 130, Loss: 0.057664\nEpoch 140, Loss: 0.059227\nEpoch 150, Loss: 0.066484\nEpoch 160, Loss: 0.058584\nEpoch 170, Loss: 0.093342\nEpoch 180, Loss: 0.057734\nEpoch 190, Loss: 0.128726\nEpoch 200, Loss: 0.051217\nEpoch 210, Loss: 0.050794\nEpoch 220, Loss: 0.049839\nTreinamento encerrado no epoch 229 (converg\u00eancia detectada).\nAccuracy: 95.33%\n\n=== Pesos e Biases do Modelo ===\n\nCamada 1:\n  Pesos W1 (shape (16, 4)):\n[[ 0.45449009 -0.81209545  0.87507731  2.18683697]\n [ 0.15004802 -1.01112088 -1.15337974 -0.09514862]\n [ 0.45935796 -0.10834158  0.70303986 -0.84685614]\n [ 0.29513792  0.05772944  0.58649792 -1.05736521]\n [ 1.6703515   0.18208112 -0.81806201 -1.72391369]\n [ 0.80567936 -0.82451442 -1.20037685 -0.09512992]\n [ 0.62185923  0.21451844 -1.55916952 -2.36866482]\n [ 1.05418959  0.38149916 -0.36325671 -0.84923767]\n [ 0.29519694  0.20959077  0.75808654 -1.12321373]\n [ 0.4891369   0.88976757  0.41393927  0.29881998]\n [ 0.12312357 -1.36602    -0.69411203 -1.17632493]\n [ 0.19580783  1.83502988 -1.02093175  1.60552112]\n [ 1.87435417 -0.05828504 -0.24050461 -0.84568474]\n [-1.60272002 -2.22218284  0.37665507 -1.51703194]\n [-1.52787748  0.50375896  0.82300988 -1.06078257]\n [ 1.70029325  0.05272135 -0.68866116 -0.77060775]]\n  Biases b1 (shape (16,)):\n[ 2.87506729 -0.97097384 -1.03208891 -1.01666649 -1.4237539  -0.35824997\n -1.38646553  1.11117524 -0.6571113   1.04982543 -2.19095566 -1.95947317\n -0.77506652 -2.87308326 -2.07890379 -0.5416115 ]\n\nCamada 2:\n  Pesos W2 (shape (8, 16)):\n[[-2.51653145  0.99919717  0.31172912  0.24975674 -0.84081024  1.10991394\n   1.43743357  0.25522576 -0.2741374  -0.14056338 -0.31440857  1.01651237\n  -0.87982114  1.1778178  -0.48786285 -0.77717119]\n [ 0.709298   -0.60106447 -0.32317526 -0.3917947   0.35926835 -0.24484754\n  -0.58428285 -0.41805447 -0.29338005 -0.05474837 -0.00984989 -0.92817564\n   0.40831344 -0.60530097  0.54390647  0.39381455]\n [ 0.24014628 -0.29199683  0.62213453  0.5360135   0.08783817 -0.76775864\n  -0.58087863  0.47373031  0.35525185 -0.05466441  1.14555677  0.51674229\n   0.60707751  0.7570584  -1.64252534 -0.13220308]\n [-0.04050464  0.09946613  0.03482787 -0.10076799  0.28732381  0.04671199\n   0.44368948  0.43547368 -0.06179928 -0.51690552  0.12609127 -0.23247329\n  -0.1159861   0.67172455 -1.30794824  0.0698007 ]\n [ 1.24695253  0.83649761 -0.57498506 -0.70753726  0.1505899   0.14347617\n  -1.44053883 -0.56842254 -0.09376871 -1.5549557  -1.03014594 -0.60825075\n   0.87014556 -1.55513437  1.74632994  0.96480407]\n [ 0.48371643 -1.55354945 -0.29322351 -0.36287776 -0.36833393 -1.0432472\n   0.34752016  0.15123585 -1.36247718  0.20055968 -0.12145993  1.73818686\n  -0.93640658 -1.61837592 -0.31875254 -0.50648797]\n [-0.71046805 -0.10557216  0.24102679 -0.14443525  0.05043702  0.64160379\n  -1.51711324 -1.26195373 -0.10748288 -0.84382939  0.59926703  1.08691862\n   0.96846356 -0.60474557  0.04744151  0.4188    ]\n [ 1.38642778 -1.24039498  0.68059487  0.52950407  1.29769176 -0.85891872\n   0.07171113 -0.90756691  0.34135117 -0.22156202 -1.4636656   0.52030564\n   0.50227455  0.73556049  0.11386068  0.98848203]]\n  Biases b2 (shape (8,)):\n[-0.18439159 -0.42846032  0.30690122 -0.22516767  0.34530631  0.84354255\n  1.55199328  0.36290337]\n\nCamada 3:\n  Pesos W3 (shape (3, 8)):\n[[ 1.72233185 -0.52519902 -0.01990436  0.33143588  0.58155462  0.12188458\n  -1.45734643 -0.83036462]\n [-2.46311209 -0.24225901  0.58411664  0.21919931 -1.52323223  1.42755525\n   1.22172185  0.39356433]\n [ 1.11033493  0.78025528 -0.49820055 -0.38504892  0.75001176 -1.86842258\n   0.20255606  0.38783064]]\n  Biases b3 (shape (3,)):\n[-0.3177156  -0.17096346  0.48867906]\n</code></pre> <p>Ainda, observa-se a imagen da matriz de confus\u00e3o que deixa em evid\u00eancia outras caracter\u00edsticas do modelos</p> <p></p> <p>Por \u00faltima, esse \u00e9 o c\u00f3digo da MLP que foi implementada:</p> <pre><code>from sklearn.metrics import confusion_matrix, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nclass mlp:\n    def __init__(self, n_features:int, n_hidden_layers: int, n_neurons_per_layer: list, \n                activation: str, loss: str, optimizer: str, epochs: int, eta: float) -&gt; None:\n        self.n_features = n_features\n        self.n_hidden_layers = n_hidden_layers\n        self.n_neurons_per_layer = n_neurons_per_layer\n        self.activation = activation\n        self.loss = loss\n        self.optimizer = optimizer\n        self.epochs = epochs\n        self.eta = eta\n\n        self.weights = []\n        self.biases = []\n        self.layer_dims = [n_features] + n_neurons_per_layer #dimens\u00f5es de cada camada, incluindo a de output\n\n        for i in range(len(self.layer_dims) - 1):\n            w = np.random.randn(self.layer_dims[i+1], self.layer_dims[i]) * 0.1\n            b = np.zeros((self.layer_dims[i+1],))\n            self.weights.append(w)\n            self.biases.append(b)\n\n        print(\"\\n=== Inicializa\u00e7\u00e3o de Pesos e Biases ===\")\n        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n            print(f\"Camada {i+1}:\")\n            print(f\"W{i+1} shape {w.shape}:\\n{w}\")\n            print(f\"b{i+1} shape {b.shape}:\\n{b}\\n\")\n\n    def train(self, X, y, threshold: float = 1e-5, window: int = 10) -&gt; None:\n        loss_history = []\n\n        for epoch in range(self.epochs):\n            total_loss = 0\n            for i in range(len(y)):\n                y_pred, cache = self.forward_pass(X[i])\n                loss = self.loss_calculation(y[i], y_pred)\n                total_loss += loss\n                grads_w, grads_b = self.backpropagation(y[i], y_pred, cache)\n                self.update_parameters(grads_w, grads_b)\n\n            avg_loss = total_loss / len(y)\n            loss_history.append(avg_loss)\n\n            if epoch % 10 == 0:\n                print(f\"Epoch {epoch}, Loss: {avg_loss:.6f}\")\n\n            # crit\u00e9rio de parada: m\u00e9dia m\u00f3vel dos \u00faltimos \"window\" epochs\n            if epoch &gt;= window:\n                moving_avg_prev = np.mean(loss_history[-2*window:-window]) #-20 at\u00e9 -10\n                moving_avg_curr = np.mean(loss_history[-window:]) # -10 at\u00e9 atual\n                if abs(moving_avg_prev - moving_avg_curr) &lt; threshold:\n                    print(f\"Treinamento encerrado no epoch {epoch} (converg\u00eancia detectada).\")\n                    break\n\n    def test(self, X: np.ndarray) -&gt; np.ndarray:\n        preds = []\n        for i in range(len(X)): \n            y_pred, _ = self.forward_pass(X[i]) #utiliza pesos j\u00e1 definidos da mlp\n            if self.loss == \"cross_entropy\":\n                preds.append(np.argmax(y_pred))  # multi-class, pega o maior do vetor\n            else:\n                preds.append(1 if y_pred &gt; 0.5 else 0)  # bin\u00e1rio\n        return np.array(preds)\n\n    def evaluate(self, X: np.ndarray, y: np.ndarray, plot_confusion: bool, plot_roc: bool, preds: np.ndarray) -&gt; None:\n        acc = self.calculate_accuracy(y, preds)\n        print(f\"Accuracy: {acc*100:.2f}%\")\n\n        print(\"\\n=== Pesos e Biases do Modelo ===\")\n        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n            print(f\"\\nCamada {i+1}:\")\n            print(f\"  Pesos W{i+1} (shape {w.shape}):\")\n            print(w)\n            print(f\"  Biases b{i+1} (shape {b.shape}):\")\n            print(b)\n\n        binary = (len(np.unique(y)) == 2)\n\n        if plot_confusion:\n            cm = confusion_matrix(y, preds)\n            cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]  # normaliza por linha\n\n            plt.figure(figsize=(6,5))\n            sns.heatmap(cm_norm, annot=cm, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n            plt.title(\"Confusion Matrix (normalized by row)\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.show()\n\n        # ROC curve (somente para bin\u00e1rio)\n        if plot_roc and binary:\n            # coletar probabilidades em vez de labels\n            y_scores = []\n            for i in range(len(X)):\n                y_pred, _ = self.forward_pass(X[i])\n                if self.loss == \"cross_entropy\":\n                    y_scores.append(y_pred[1])  # probabilidade da classe 1\n                else:\n                    y_scores.append(y_pred)     # sa\u00edda do sigmoid\n            y_scores = np.array(y_scores)\n\n            fpr, tpr, _ = roc_curve(y, y_scores)\n            roc_auc = auc(fpr, tpr)\n\n            plt.figure()\n            plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n            plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n            plt.xlabel(\"False Positive Rate\")\n            plt.ylabel(\"True Positive Rate\")\n            plt.title(\"Receiver Operating Characteristic (ROC)\")\n            plt.legend(loc=\"lower right\")\n            plt.show()\n\n    def calculate_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        return np.mean(y_true == y_pred)\n\n    def forward_pass(self, x: np.ndarray) -&gt; tuple:\n        a = x\n        cache = {\"z\": [], \"a\": [a]}  # salva ativa\u00e7\u00f5es\n        for i in range(len(self.weights)):\n            z = np.dot(self.weights[i], a) + self.biases[i]\n            if i == len(self.weights) - 1 and self.loss == \"cross_entropy\":\n                a = self.softmax(z)\n            else:\n                a = self.activation_function(z)\n            cache[\"z\"].append(z)\n            cache[\"a\"].append(a)\n        return a, cache\n\n    def backpropagation(self, y_true: np.ndarray, y_pred: np.ndarray, cache: dict) -&gt; tuple:\n        grads_w = [None] * len(self.weights)\n        grads_b = [None] * len(self.biases)\n\n        # \u00daltima camada\n        if self.loss == \"cross_entropy\":\n            delta = self.derive_cross_entropy(y_true, y_pred)\n        elif self.loss == \"mse\":\n            dloss_dy_pred = self.derive_mse(y_true, y_pred)\n            if self.activation == \"sigmoid\":\n                delta = dloss_dy_pred * self.derive_sigmoid(cache[\"z\"][-1])\n            elif self.activation == \"tanh\":\n                delta = dloss_dy_pred * self.derive_tanh(cache[\"z\"][-1])\n            elif self.activation == \"relu\":\n                delta = dloss_dy_pred * self.derive_relu(cache[\"z\"][-1])\n        else:\n            raise ValueError(\"Loss n\u00e3o suportada\")\n\n        grads_w[-1] = np.outer(delta, cache[\"a\"][-2])\n        grads_b[-1] = delta\n\n        # Camadas ocultas\n        for l in reversed(range(len(self.weights)-1)):\n            delta = np.dot(self.weights[l+1].T, delta)\n            if self.activation == \"sigmoid\":\n                delta *= self.derive_sigmoid(cache[\"z\"][l])\n            elif self.activation == \"tanh\":\n                delta *= self.derive_tanh(cache[\"z\"][l])\n            elif self.activation == \"relu\":\n                delta *= self.derive_relu(cache[\"z\"][l])\n            grads_w[l] = np.outer(delta, cache[\"a\"][l])\n            grads_b[l] = delta\n\n        return grads_w, grads_b\n\n    def update_parameters(self, grads_w, grads_b):\n        if self.optimizer == \"gd\":  # Gradient Descent padr\u00e3o\n            for i in range(len(self.weights)):\n                self.weights[i] -= self.eta * grads_w[i]\n                self.biases[i]  -= self.eta * grads_b[i]\n        else:\n            raise ValueError(f\"Optimizer {self.optimizer} n\u00e3o suportado\")\n\n    def loss_calculation(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        if self.loss == 'mse':\n            return self.mse(y_true, y_pred)\n        elif self.loss == 'cross_entropy':\n            return self.cross_entropy(y_true, y_pred)\n        else:\n            raise ValueError(f\"Fun\u00e7\u00e3o de loss {self.loss} n\u00e3o suportada\")\n\n    def activation_function(self, z: np.ndarray) -&gt; np.ndarray:\n        if self.activation == 'sigmoid':\n            return self.sigmoid(z)\n        elif self.activation == 'tanh':\n            return self.tanh(z)\n        elif self.activation == 'relu':\n            return self.relu(z)\n        else:\n            raise ValueError(f\"Fun\u00e7\u00e3o de ativa\u00e7\u00e3o {self.activation} n\u00e3o suportada\")\n\n    def mse(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        return np.mean((y_true - y_pred)**2)\n\n    def derive_mse(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        return -2*(y_true - y_pred)\n\n    def cross_entropy(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        num_classes = len(y_pred)\n        y_true_onehot = np.eye(num_classes)[y_true]\n        eps = 1e-15\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n        return -np.sum(y_true_onehot * np.log(y_pred))\n\n    def derive_cross_entropy(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:\n        num_classes = len(y_pred)\n        y_true_onehot = np.eye(num_classes)[y_true]\n        return y_pred - y_true_onehot\n\n    def sigmoid(self, z: np.ndarray) -&gt; np.ndarray:\n        return 1 / (1 + np.exp(-z))\n\n    def derive_sigmoid(self, z: np.ndarray) -&gt; np.ndarray:\n        s = self.sigmoid(z)\n        return s * (1 - s)\n\n    def tanh(self, z: np.ndarray) -&gt; np.ndarray:\n        return np.tanh(z)\n\n    def derive_tanh(self, z: np.ndarray) -&gt; np.ndarray:\n        return 1 - (np.tanh(z))**2\n\n    def relu(self, z: np.ndarray) -&gt; np.ndarray:\n        return np.maximum(0, z)\n\n    def derive_relu(self, z: np.ndarray) -&gt; np.ndarray:\n        return (z &gt; 0).astype(float)\n\n    def softmax(self, z: np.ndarray) -&gt; np.ndarray:\n        exp_z = np.exp(z - np.max(z))\n        return exp_z / np.sum(exp_z)\n</code></pre>"},{"location":"exercicio4/main/","title":"Exercicio 4","text":""},{"location":"exercicio4/main/#exercicio-4-ainda-nao-foi-feito","title":"Exerc\u00edcio 4 ainda n\u00e3o foi feito","text":""},{"location":"projeto/main/","title":"Main","text":"<p>Projeto 1 feito</p>"},{"location":"projeto/projeto1/main/","title":"Projeto 1","text":""},{"location":"projeto/projeto1/main/#classificacao-satisfacao-de-passageiro-em-linhas-aereas","title":"Classifica\u00e7\u00e3o - Satisfa\u00e7\u00e3o de Passageiro em Linhas A\u00e9reas","text":""},{"location":"projeto/projeto1/main/#autores","title":"Autores","text":"<ul> <li>Felipe Bakowski Nantes de Souza  </li> <li>Vinicius Grecco Fonseca Mulato  </li> <li>Victor Soares</li> </ul>"},{"location":"projeto/projeto1/main/#nota","title":"Nota:","text":"<ul> <li>O c\u00f3gido foi feito considerando um arquivo auxiliar mlp.py e um outro utils.py. Aquele possui a implementa\u00e7\u00e3o completa da mlp e este fun\u00e7\u00f5es auxiliares que ajudam na organiza\u00e7\u00e3o</li> </ul>"},{"location":"projeto/projeto1/main/#1-data-set-selecao","title":"1.    Data set - Sele\u00e7\u00e3o","text":""},{"location":"projeto/projeto1/main/#data-set-escolhido-analise-de-satisfacao-de-passageiros-em-voos-kaggle","title":"Data set escolhido: An\u00e1lise de satisfa\u00e7\u00e3o de passageiros em voos, Kaggle","text":"<p>Esse data set devido a uma soma de alguns fatores. De in\u00edcio, ele representa um desafio real que muitas empresas precisam resolver e \u00e9 interessante interagir com um problema que tenham reflex\u00f5es reais. Ainda, ele possui grande n\u00famero de linhas, proporcionando uma an\u00e1lise mais robusta, muitas features de todos os tipos e uma vari\u00e1vel target bem balanceada. Criando assim, um terreno f\u00e9rtil para a an\u00e1lise de dados.</p> <ul> <li>Fonte: https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction</li> </ul>"},{"location":"projeto/projeto1/main/#2-data-set-explicacao","title":"2. Data set - Explica\u00e7\u00e3o","text":"<p>Esse data set possui 25 colunas, 24 sendo potenciais features e 1 target (satisfa\u00e7\u00e3o). Ela \u00e9 uma vari\u00e1vel categ\u00f3rica n\u00e3o ordenada e assume 2 valores: satisfeito ou neutro/insatisfeito. Ainda, a target est\u00e1 bem balanceada, estando divida em 43/57 %</p> <p>Em rela\u00e7\u00e3o as features, elas variam entre qualitativas e quantitativas, sendo majoritariamente qualitativas e com poucas linhas com valores faltando.</p>"},{"location":"projeto/projeto1/main/#colunas","title":"Colunas","text":"<p>Agora, pode-se observar um c\u00f3digo que concatena as bases de treino e teste, visualiza o n\u00famero de linhas totais e cita todas as colunas que possuem no dataframe</p> <pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mlp import mlp\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom utils import plot_distributions\n\n# carregar os dois conjuntos\ndf_train = pd.read_csv(\"train.csv\")\ndf_test  = pd.read_csv(\"test.csv\")\n\n# marcar a origem para poder separar depois\ndf_train[\"_source\"] = \"train\"\ndf_test[\"_source\"]  = \"test\"\n\n# concatenar\ndf = pd.concat([df_train, df_test], ignore_index=True)\n\nprint(\"Shape combinado:\", df.shape)\nprint(df[\"_source\"].value_counts())\n\ntarget = df['satisfaction']\n\ndf.columns\n</code></pre>"},{"location":"projeto/projeto1/main/#output","title":"Output","text":"<pre><code>Shape combinado: (129880, 26)\n_source\ntrain    103904\ntest      25976\nName: count, dtype: int64\nIndex(['Unnamed: 0', 'id', 'Gender', 'Customer Type', 'Age', 'Type of Travel',\n       'Class', 'Flight Distance', 'Inflight wifi service',\n       'Departure/Arrival time convenient', 'Ease of Online booking',\n       'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n       'Inflight entertainment', 'On-board service', 'Leg room service',\n       'Baggage handling', 'Checkin service', 'Inflight service',\n       'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes',\n       'satisfaction', '_source'],\n      dtype='object')\n</code></pre>"},{"location":"projeto/projeto1/main/#visualizando-equilibrio-na-variavel-target-satisfaction","title":"Visualizando equil\u00edbrio na vari\u00e1vel Target (satisfaction)","text":"<pre><code>plt.figure(figsize=(6,4))\nax = sns.histplot(x=target, stat=\"percent\", discrete=True)\n\nplt.title(\"Distribui\u00e7\u00e3o da vari\u00e1vel 'satisfaction' (%)\")\nplt.xlabel(\"Satisfa\u00e7\u00e3o\")\nplt.ylabel(\"Porcentagem\")\nplt.show()\n</code></pre>"},{"location":"projeto/projeto1/main/#preenchendo-valores-vazios","title":"Preenchendo valores vazios","text":"<pre><code>print(df.columns[df.isnull().any()]) #observa-se colunas com valores faltando!\n</code></pre> <pre><code>Index(['Arrival Delay in Minutes'], dtype='object')\n</code></pre>"},{"location":"projeto/projeto1/main/#a-variavel-arrival-delay-e-numerica-para-tratar-os-valores-ausentes-nessa-coluna-o-grupo-considerou-que-a-ausencia-de-registros-poderia-estar-relacionada-a-um-esquecimento-por-parte-da-tripulacao-ou-da-companhia-aerea-ao-preencher-o-dado-possivelmente-por-nao-ter-havido-atraso-caso-um-atraso-tivesse-ocorrido-o-preenchimento-dessa-informacao-seria-mais-relevante-e-portanto-menos-provavel-de-ser-omitido-assim-optou-se-por-substituir-os-valores-ausentes-por-zero-que-alem-de-representar-a-ausencia-de-atraso-corresponde-tambem-a-moda-da-variavel","title":"A vari\u00e1vel Arrival Delay \u00e9 num\u00e9rica. Para tratar os valores ausentes nessa coluna, o grupo considerou que a aus\u00eancia de registros poderia estar relacionada a um esquecimento por parte da tripula\u00e7\u00e3o ou da companhia a\u00e9rea ao preencher o dado, possivelmente por n\u00e3o ter havido atraso. Caso um atraso tivesse ocorrido, o preenchimento dessa informa\u00e7\u00e3o seria mais relevante e, portanto, menos prov\u00e1vel de ser omitido. Assim, optou-se por substituir os valores ausentes por zero, que, al\u00e9m de representar a aus\u00eancia de atraso, corresponde tamb\u00e9m \u00e0 moda da vari\u00e1vel.","text":"<pre><code>df['Arrival Delay in Minutes'] = df['Arrival Delay in Minutes'].fillna(df['Arrival Delay in Minutes'].mode()[0])\nprint(df.columns[df.isnull().any()]) #roda mais uma vez para garantir\n</code></pre> <pre><code>Index([], dtype='object')\n</code></pre>"},{"location":"projeto/projeto1/main/#visualizacao-features","title":"Visualiza\u00e7\u00e3o features","text":"<pre><code># definir colunas\nquantitative_cols = [\"Age\", \"Flight Distance\", \"Departure Delay in Minutes\", \"Arrival Delay in Minutes\"]\n\nordinal_cols = [\n    \"Inflight wifi service\",\n    \"Departure/Arrival time convenient\",\n    \"Ease of Online booking\",\n    \"Gate location\",\n    \"Food and drink\",\n    \"Online boarding\",\n    \"Seat comfort\",\n    \"Inflight entertainment\",\n    \"On-board service\",\n    \"Leg room service\",\n    \"Baggage handling\",\n    \"Checkin service\",\n    \"Inflight service\",\n    \"Cleanliness\",\n]\n\nnominal_cols = [\"Gender\", \"Customer Type\", \"Type of Travel\", \"Class\"]\ntarget_col = \"satisfaction\"\n\n# -------- PLOT ORIGINAL --------\nplot_distributions(df, quantitative_cols, ordinal_cols, nominal_cols, title_suffix=\"(Original)\", force_ordinal_continuous=False)\n</code></pre> <p>Pelas distribui\u00e7\u00f5es apresentadas, as vari\u00e1veis categ\u00f3ricas est\u00e3o relativamente balanceadas em alguns aspectos, mas apresentam diferen\u00e7as relevantes em outros. O g\u00eanero est\u00e1 equilibrado entre homens e mulheres. J\u00e1 o tipo de cliente \u00e9 bastante desbalanceado, com predomin\u00e2ncia de clientes leais. No tipo de viagem, h\u00e1 mais viagens de neg\u00f3cios do que pessoais. Em rela\u00e7\u00e3o \u00e0 classe, as categorias Business e Eco t\u00eam propor\u00e7\u00f5es pr\u00f3ximas, enquanto Eco Plus aparece em bem menor quantidade. A vari\u00e1vel de satisfa\u00e7\u00e3o tamb\u00e9m \u00e9 relativamente balanceada, com uma leve maioria de clientes insatisfeitos ou neutros.</p> <p>Nas vari\u00e1veis num\u00e9ricas, observa-se diversidade: a idade segue uma distribui\u00e7\u00e3o concentrada entre 20 e 50 anos; a dist\u00e2ncia do voo \u00e9 enviesada para valores menores; e os atrasos de partida e chegada apresentam forte concentra\u00e7\u00e3o em atrasos curtos, com alguns outliers de longos atrasos. J\u00e1 os servi\u00e7os avaliativos (como wifi, comida, embarque, conforto de assento, limpeza, etc.) mostram distribui\u00e7\u00f5es variadas, mas tendem a concentrar respostas em notas intermedi\u00e1rias a altas, o que sugere certo vi\u00e9s positivo nas avalia\u00e7\u00f5es.</p>"},{"location":"projeto/projeto1/main/#3-limpeza-de-dados-e-normalizacao","title":"3. Limpeza de dados e normaliza\u00e7\u00e3o","text":""},{"location":"projeto/projeto1/main/#z-score-media-0-desvio-1","title":"Z-Score (m\u00e9dia 0, desvio 1)","text":"<p>Usamos em Age e nas vari\u00e1veis ordinais (avalia\u00e7\u00f5es de 1 a 5).</p> <ul> <li> <p>Por qu\u00ea?</p> </li> <li> <p>O Z-score centraliza os dados na m\u00e9dia e escala pela variabilidade.</p> </li> <li>Isso coloca todas essas vari\u00e1veis em uma escala compar\u00e1vel (valores entre -2 e 2, geralmente).</li> <li>\u00c9 \u00fatil quando os dados s\u00e3o aproximadamente sim\u00e9tricos ou queremos destacar desvios em rela\u00e7\u00e3o \u00e0 m\u00e9dia.</li> </ul> <p>Exemplo: Idades diferentes s\u00e3o comparadas em termos de \"quantos desvios padr\u00e3o acima ou abaixo da m\u00e9dia\" est\u00e3o.</p>"},{"location":"projeto/projeto1/main/#min-max-1-1","title":"Min-Max [-1, 1]","text":"<p>Usamos em Flight Distance e nos delays (ap\u00f3s log).</p> <ul> <li> <p>Por qu\u00ea?</p> </li> <li> <p>O Min-Max traz os valores para um intervalo fixo, aqui entre -1 e 1.</p> </li> <li>Isso garante que nenhuma vari\u00e1vel tenha escala muito maior que as outras.</li> <li>\u00c9 \u00fatil quando a distribui\u00e7\u00e3o n\u00e3o \u00e9 centrada na m\u00e9dia, mas queremos que o modelo \u201cveja\u201d tudo na mesma faixa.</li> </ul>"},{"location":"projeto/projeto1/main/#log-min-max","title":"Log + Min-Max","text":"<p>Usamos em Departure Delay e Arrival Delay.</p> <ul> <li> <p>Por qu\u00ea?</p> </li> <li> <p>Atrasos t\u00eam distribui\u00e7\u00e3o muito enviesada: muitos voos com atraso 0 ou baixo, e poucos voos com atrasos enormes.</p> </li> <li>O log \u201ccomprime\u201d esses valores grandes, reduzindo o impacto dos extremos.</li> <li>Depois, aplicamos Min-Max para trazer o resultado para a faixa [-1,1], alinhando com as outras features.</li> </ul>"},{"location":"projeto/projeto1/main/#one-hot-encoding-nominais","title":"One-Hot Encoding (nominais)","text":"<p>Nas vari\u00e1veis como Gender, Customer Type, Type of Travel, Class.</p> <ul> <li> <p>Por qu\u00ea?</p> </li> <li> <p>S\u00e3o categorias sem ordem (ex.: \u201cMale\u201d \u2260 maior que \u201cFemale\u201d).</p> </li> <li>O One-Hot cria colunas bin\u00e1rias (<code>0</code> ou <code>1</code>) para cada categoria, sem necessidade de normaliza\u00e7\u00e3o extra.</li> </ul>"},{"location":"projeto/projeto1/main/#implementando-a-normalizacao","title":"Implementando a normaliza\u00e7\u00e3o","text":"<pre><code># -------- ONE-HOT + TARGET --------\ndf_encoded = pd.get_dummies(df, columns=nominal_cols, dtype=int)\ndf_encoded[target_col] = df_encoded[target_col].map({\n    \"satisfied\": 1,\n    \"neutral or dissatisfied\": 0\n})\n\n# -------- NORMALIZA\u00c7\u00c3O --------\ndf_norm = df_encoded.copy()\n\n# Age + ordinais -&gt; Z-score\nscaler_z = StandardScaler()\ndf_norm[[\"Age\"] + ordinal_cols] = scaler_z.fit_transform(df_norm[[\"Age\"] + ordinal_cols])\n\n# Flight Distance -&gt; MinMax [-1,1]\nscaler_fd = MinMaxScaler(feature_range=(-1,1))\ndf_norm[\"Flight Distance\"] = scaler_fd.fit_transform(df_norm[[\"Flight Distance\"]])\n\n# Delays -&gt; log + MinMax [-1,1]\nfor col in [\"Departure Delay in Minutes\", \"Arrival Delay in Minutes\"]:\n    df_norm[col] = np.log1p(df_norm[col].clip(lower=0))\n    scaler_delay = MinMaxScaler(feature_range=(-1,1))\n    df_norm[col] = scaler_delay.fit_transform(df_norm[[col]])\n\ndf_norm.head()\n# -------- PLOT NORMALIZADO --------\nplot_distributions(\n    df_norm,\n    quantitative_cols,\n    ordinal_cols,\n    [],  # &lt;- passa lista vazia, n\u00e3o plota nominais\n    title_suffix=\"(Normalizado)\",\n    force_ordinal_continuous=True\n)\n</code></pre>"},{"location":"projeto/projeto1/main/#4-implementacao-mlp","title":"4. Implementa\u00e7\u00e3o MLP","text":"<pre><code>from sklearn.metrics import confusion_matrix, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nclass mlp:\n    def __init__(self, n_features:int, n_hidden_layers: int, n_neurons_per_layer: list, \n                activation: str, loss: str, optimizer: str, epochs: int, eta: float) -&gt; None:\n        self.n_features = n_features\n        self.n_hidden_layers = n_hidden_layers\n        self.n_neurons_per_layer = n_neurons_per_layer\n        self.activation = activation\n        self.loss = loss\n        self.optimizer = optimizer\n        self.epochs = epochs\n        self.eta = eta\n\n        self.weights = []\n        self.biases = []\n        self.layer_dims = [n_features] + n_neurons_per_layer\n\n        for i in range(len(self.layer_dims) - 1):\n            w = np.random.randn(self.layer_dims[i+1], self.layer_dims[i]) * 0.1\n            b = np.zeros((self.layer_dims[i+1],))\n            self.weights.append(w)\n            self.biases.append(b)\n\n        # hist\u00f3rico de treino\n        self.history = {\"loss\": [], \"accuracy\": []}\n\n        print(\"\\n=== Inicializa\u00e7\u00e3o de Pesos e Biases ===\")\n        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n            print(f\"Camada {i+1}:\")\n            print(f\"W{i+1} shape {w.shape}:\\n{w}\")\n            print(f\"b{i+1} shape {b.shape}:\\n{b}\\n\")\n\n    def train(self, X, y, threshold: float = 5e-3, window: int = 10) -&gt; None:\n        loss_history = []\n\n        for epoch in range(self.epochs):\n            total_loss = 0\n            for i in range(len(y)):\n                y_pred, cache = self.forward_pass(X[i])\n                loss = self.loss_calculation(y[i], y_pred)\n                total_loss += loss\n                grads_w, grads_b = self.backpropagation(y[i], y_pred, cache)\n                self.update_parameters(grads_w, grads_b)\n\n            avg_loss = total_loss / len(y)\n            loss_history.append(avg_loss)\n\n            preds_train = self.test(X)\n            train_acc = self.calculate_accuracy(y, preds_train)\n\n            # armazenar no hist\u00f3rico\n            self.history[\"loss\"].append(avg_loss)\n            self.history[\"accuracy\"].append(train_acc)\n\n            if epoch % 10 == 0:\n                acc_str = f\"{train_acc*100:.2f}%\" if not np.isnan(train_acc) else \"nan\"\n                print(f\"Epoch {epoch}, Loss: {avg_loss:.6f}, Train Acc: {acc_str}\")\n\n            # crit\u00e9rio de parada: m\u00e9dia m\u00f3vel dos \u00faltimos \"window\" epochs\n            if epoch &gt;= window:\n                moving_avg_prev = np.mean(loss_history[-2*window:-window])\n                moving_avg_curr = np.mean(loss_history[-window:])\n                if abs(moving_avg_prev - moving_avg_curr) &lt; threshold:\n                    print(f\"Treinamento encerrado no epoch {epoch} (converg\u00eancia detectada).\")\n                    break\n\n    def test(self, X: np.ndarray) -&gt; np.ndarray:\n        preds = []\n        for i in range(len(X)): \n            y_pred, _ = self.forward_pass(X[i])\n            if self.loss == \"cross_entropy\":\n                preds.append(np.argmax(y_pred))\n            else:\n                val = np.array(y_pred).ravel()\n                v = val[0] if val.size &gt; 0 else val\n                preds.append(1 if v &gt; 0.5 else 0)\n        return np.array(preds)\n\n\n    def evaluate(self, X: np.ndarray, y: np.ndarray, plot_confusion: bool, plot_roc: bool, preds: np.ndarray) -&gt; None:\n        acc = self.calculate_accuracy(y, preds)\n        print(f\"Accuracy: {acc*100:.2f}%\")\n\n        print(\"\\n=== Pesos e Biases do Modelo ===\")\n        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n            print(f\"\\nCamada {i+1}:\")\n            print(f\"  Pesos W{i+1} (shape {w.shape}):\")\n            print(w)\n            print(f\"  Biases b{i+1} (shape {b.shape}):\")\n            print(b)\n\n        binary = (len(np.unique(y)) == 2)\n\n        if plot_confusion:\n            self.plot_confusion_matrix(y, preds)\n\n        if plot_roc and binary:\n            self.plot_roc_curve(X, y)\n\n        # sempre plota hist\u00f3rico se existir\n        if self.history and len(self.history.get(\"loss\", [])) &gt; 0:\n            self.plot_history()\n\n    # -------- Fun\u00e7\u00f5es auxiliares de plot --------\n    def plot_confusion_matrix(self, y_true, y_pred):\n        cm = confusion_matrix(y_true, y_pred)\n        cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n\n        plt.figure(figsize=(6,5))\n        sns.heatmap(cm_norm, annot=cm, fmt=\"d\", cmap=\"Blues\",\n                    xticklabels=np.unique(y_true),\n                    yticklabels=np.unique(y_true))\n        plt.title(\"Confusion Matrix (normalized by row)\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.show()\n\n    def plot_roc_curve(self, X, y_true):\n        y_scores = []\n        for i in range(len(X)):\n            y_pred, _ = self.forward_pass(X[i])\n            if self.loss == \"cross_entropy\":\n                y_scores.append(y_pred[1])\n            else:\n                val = np.array(y_pred).ravel()\n                y_scores.append(val[0] if val.size &gt; 0 else val)\n        y_scores = np.array(y_scores)\n\n        fpr, tpr, _ = roc_curve(y_true, y_scores)\n        roc_auc = auc(fpr, tpr)\n\n        plt.figure()\n        plt.plot(fpr, tpr, color=\"darkorange\", lw=2,\n                 label=f\"ROC curve (area = {roc_auc:.2f})\")\n        plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n        plt.xlabel(\"False Positive Rate\")\n        plt.ylabel(\"True Positive Rate\")\n        plt.title(\"Receiver Operating Characteristic (ROC)\")\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\n    def plot_history(self):\n        \"\"\"Plota loss e accuracy armazenados em self.history.\"\"\"\n        loss = self.history.get(\"loss\", [])\n        acc = self.history.get(\"accuracy\", [])\n        epochs = np.arange(1, len(loss)+1)\n\n        fig, axes = plt.subplots(1, 2, figsize=(12,4))\n        # loss\n        axes[0].plot(epochs, loss, marker=\"o\")\n        axes[0].set_title(\"Loss por Epoch\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Loss\")\n        axes[0].grid(True, linestyle=\"--\", alpha=0.4)\n        # accuracy\n        axes[1].plot(epochs, acc, marker=\"o\")\n        axes[1].set_title(\"Accuracy por Epoch (treino)\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Accuracy\")\n        axes[1].grid(True, linestyle=\"--\", alpha=0.4)\n\n        plt.tight_layout()\n        plt.show()\n\n    def calculate_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        return np.mean(y_true == y_pred)\n\n    def forward_pass(self, x: np.ndarray) -&gt; tuple:\n        a = x\n        cache = {\"z\": [], \"a\": [a]}\n        for i in range(len(self.weights)):\n            z = np.dot(self.weights[i], a) + self.biases[i]\n            if i == len(self.weights) - 1 and self.loss == \"cross_entropy\":\n                a = self.softmax(z)\n            else:\n                a = self.activation_function(z)\n            cache[\"z\"].append(z)\n            cache[\"a\"].append(a)\n        return a, cache\n\n    def backpropagation(self, y_true: np.ndarray, y_pred: np.ndarray, cache: dict) -&gt; tuple:\n        grads_w = [None] * len(self.weights)\n        grads_b = [None] * len(self.biases)\n\n        # \u00daltima camada\n        if self.loss == \"cross_entropy\":\n            delta = self.derive_cross_entropy(y_true, y_pred)\n        elif self.loss == \"mse\":\n            dloss_dy_pred = self.derive_mse(y_true, y_pred)\n            if self.activation == \"sigmoid\":\n                delta = dloss_dy_pred * self.derive_sigmoid(cache[\"z\"][-1])\n            elif self.activation == \"tanh\":\n                delta = dloss_dy_pred * self.derive_tanh(cache[\"z\"][-1])\n            elif self.activation == \"relu\":\n                delta = dloss_dy_pred * self.derive_relu(cache[\"z\"][-1])\n        else:\n            raise ValueError(\"Loss n\u00e3o suportada\")\n\n        grads_w[-1] = np.outer(delta, cache[\"a\"][-2])\n        grads_b[-1] = delta\n\n        # Camadas ocultas\n        for l in reversed(range(len(self.weights)-1)):\n            delta = np.dot(self.weights[l+1].T, delta)\n            if self.activation == \"sigmoid\":\n                delta *= self.derive_sigmoid(cache[\"z\"][l])\n            elif self.activation == \"tanh\":\n                delta *= self.derive_tanh(cache[\"z\"][l])\n            elif self.activation == \"relu\":\n                delta *= self.derive_relu(cache[\"z\"][l])\n            grads_w[l] = np.outer(delta, cache[\"a\"][l])\n            grads_b[l] = delta\n\n        return grads_w, grads_b\n\n    def update_parameters(self, grads_w, grads_b):\n        if self.optimizer == \"gd\":\n            for i in range(len(self.weights)):\n                self.weights[i] -= self.eta * grads_w[i]\n                self.biases[i]  -= self.eta * grads_b[i]\n        else:\n            raise ValueError(f\"Optimizer {self.optimizer} n\u00e3o suportado\")\n\n    def loss_calculation(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        if self.loss == 'mse':\n            return self.mse(y_true, y_pred)\n        elif self.loss == 'cross_entropy':\n            return self.cross_entropy(y_true, y_pred)\n        else:\n            raise ValueError(f\"Fun\u00e7\u00e3o de loss {self.loss} n\u00e3o suportada\")\n\n    def activation_function(self, z: np.ndarray) -&gt; np.ndarray:\n        if self.activation == 'sigmoid':\n            return self.sigmoid(z)\n        elif self.activation == 'tanh':\n            return self.tanh(z)\n        elif self.activation == 'relu':\n            return self.relu(z)\n        else:\n            raise ValueError(f\"Fun\u00e7\u00e3o de ativa\u00e7\u00e3o {self.activation} n\u00e3o suportada\")\n\n    def mse(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        return np.mean((y_true - y_pred)**2)\n\n    def derive_mse(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:\n        return -2*(y_true - y_pred)\n\n    def cross_entropy(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        num_classes = len(y_pred)\n        y_true_onehot = np.eye(num_classes)[y_true]\n        eps = 1e-15\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n        return -np.sum(y_true_onehot * np.log(y_pred))\n\n    def derive_cross_entropy(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:\n        num_classes = len(y_pred)\n        y_true_onehot = np.eye(num_classes)[y_true]\n        return y_pred - y_true_onehot\n\n    def sigmoid(self, z: np.ndarray) -&gt; np.ndarray:\n        return 1 / (1 + np.exp(-z))\n\n    def derive_sigmoid(self, z: np.ndarray) -&gt; np.ndarray:\n        s = self.sigmoid(z)\n        return s * (1 - s)\n\n    def tanh(self, z: np.ndarray) -&gt; np.ndarray:\n        return np.tanh(z)\n\n    def derive_tanh(self, z: np.ndarray) -&gt; np.ndarray:\n        return 1 - (np.tanh(z))**2\n\n    def relu(self, z: np.ndarray) -&gt; np.ndarray:\n        return np.maximum(0, z)\n\n    def derive_relu(self, z: np.ndarray) -&gt; np.ndarray:\n        return (z &gt; 0).astype(float)\n\n    def softmax(self, z: np.ndarray) -&gt; np.ndarray:\n        exp_z = np.exp(z - np.max(z))\n        return exp_z / np.sum(exp_z)\n</code></pre>"},{"location":"projeto/projeto1/main/#observe","title":"Observe","text":"<p>Essa implementa\u00e7\u00e3o de MLP permite que o usu\u00e1rio escolha o n\u00famero de layers e seu tamanho (output tamb\u00e9m), permite que o usu\u00e1rio escolhe entre Relu, sigmoid ou tanh como fun\u00e7\u00e3o de ativa\u00e7\u00e3o, disponibiliza cross-entropy ou MSE para calculo de erro, mas tem a limita\u00e7\u00e3o de que s\u00f3 permite o usu\u00e1rio escolher o Gradient Descent padr\u00e3o (GD) como otimiza\u00e7\u00e3o, mas como ele trabalha em modo estoc\u00e1stico (atualiza peso em cada amostra), pode-se considerar um SGD.</p> <p>Por fim, existem alguns par\u00e2metros chaves que podem ser alterados na chamada da classe que s\u00e3o cruciais para o funcionamento da rede neural. Eles s\u00e3o: learning rate ('eta' no c\u00f3digo), epochs e o n\u00famero de camadas ocultas, assim como o n\u00famero de neur\u00f4nios nela.</p>"},{"location":"projeto/projeto1/main/#5-treinando-modelo","title":"5. Treinando Modelo","text":"<p>Assim, temos a nossa MLP pronta, agora basta implementar, para isso utilizamos os seguintes par\u00e2metros:</p> <p></p><pre><code>model = mlp(\n    n_features=X_train.shape[1],\n    n_hidden_layers=2,\n    n_neurons_per_layer=[32, 16, 2],\n    activation=\"relu\",\n    loss=\"mse\",\n    optimizer=\"gd\",\n    epochs=100,\n    eta=0.01\n)\n\nmodel.train(X_train, y_train)\n</code></pre> Quando fazemos model = mlp(...) os pesos e biases j\u00e1 s\u00e3o inicializados automaticamente:<p></p> <pre><code>=== Inicializa\u00e7\u00e3o de Pesos e Biases ===\nCamada 1:\nW1 shape (32, 27):\n[[ 1.15933603e-01 -1.81859311e-02  3.78720797e-02  1.61887445e-01\n  -1.76264976e-02 -1.17896559e-01  2.29659597e-01  8.47392716e-02\n   7.84833501e-02  1.17116376e-01 -3.29254012e-02  1.47752258e-01\n  -5.05408257e-02 -1.03760407e-02 -6.75068207e-02  7.65876531e-02\n   2.24685014e-01 -4.85771329e-02  9.20202828e-02 -1.39268909e-01\n  -1.33149834e-01  7.09939469e-02  9.62322284e-03  1.97430345e-01\n   7.96567200e-02 -2.02657840e-01  9.01385734e-03]\n [-6.04410809e-02  1.55657322e-02 -6.76827315e-02  8.16054968e-02\n   9.60520887e-03 -1.25204605e-01 -6.19064575e-02 -6.01804224e-02\n   5.45090162e-02  2.06770711e-01  8.75548772e-02  7.76242877e-02\n   9.78846571e-03 -3.32076578e-02 -2.81371297e-02 -1.04293824e-01\n  -2.11831322e-01  9.67483947e-02  1.19716602e-02  4.71019153e-02\n   8.49512245e-03 -9.58052260e-02  1.06920949e-01  1.34581558e-01\n   1.10775335e-01 -1.21445693e-02 -8.24293321e-03]\n [-1.07644203e-01  1.19474816e-01 -1.23425620e-02 -4.76688842e-02\n  -1.30960013e-01  8.46369514e-02 -2.03250433e-02 -4.67880145e-02\n  -3.55089096e-02  1.30250049e-01  1.03907897e-02  1.98612609e-01\n  -9.29014497e-02 -3.78056341e-02  9.92537078e-02  6.86785045e-02\n   1.28081217e-01  3.12844113e-02 -1.04421485e-01 -1.45493638e-01\n   2.50604195e-01 -1.04281028e-01  1.41494095e-02 -1.53304959e-02\n  -2.31537260e-01 -5.91761023e-03 -6.21277588e-02]\n...\n   0.09038222 -0.10695692 -0.02021934 -0.06621559]]\nb3 shape (2,):\n[0. 0.]\n</code></pre> <p>Observe que escolhemos cross_entropy como fun\u00e7\u00e3o de c\u00e1lculo de loss, devido a isso, devemos ter um output de 2 neur\u00f4nios, cada um deles diz a chance de uma amostra pertencer a uma classe. Mas a MLP formata isso de modo que no final o resultado fica apenas 1 ou 0 para compara\u00e7\u00e3o nos testes</p> <p>Quando usamos model.train, a mlp j\u00e1 toma conta do treinamento sozinho, fazendo todo o processo de forward propagation, loss calculation, backpropagation e atualiza\u00e7\u00e3o de par\u00e2metros. O \u00fanico problema que encontrei no treinamento foi uma demora excessiva na converg\u00eancia, isso pode ser explicado pelo eta baixo que foi escolhido. Mas, a solu\u00e7\u00e3o encontrada foi atualizar o threshold de parada para um valor menor, sem prejudicar a qualidade do modelo.</p>"},{"location":"projeto/projeto1/main/#6-estrategia-de-treino-e-teste","title":"6. Estrat\u00e9gia de treino e teste","text":"<p>O dataset foi extra\u00eddo do Kaggle j\u00e1 em 2 se\u00e7\u00f5es, treino e teste (75/25), como pode ser observado na primeira se\u00e7\u00e3o desse documento</p> <p>Em rela\u00e7\u00e3o ao modo de treino foi feito um treinamento em modo estoc\u00e1stico, ou seja, atualiza os pesos em cada amostra. Isso \u00e9 \u00fatil para esse caso, pois esse \u00e9 o m\u00e9todo mais r\u00e1pido para datasets grandes (100K+ linhas).</p> <p>Ainda, \u00e9 implementado um m\u00e9todo de early stopping para evitir o overfitting, basicamente, analisamos o loss m\u00e9dio de uma janela (par\u00e2metro ajust\u00e1vel) de epochs e verificamos se a o loss atual menos o loss m\u00e9dio dessa janela \u00e9 menor que um threshold de parada, caso sim, isso indica converg\u00eancia. Tendo isso em vista, pode-se para o modelo j\u00e1 para evitar o overfitting.</p> <p>Converg\u00eancia:</p> <pre><code>Epoch 0, Loss: 0.162550, Train Acc: 94.72%\nEpoch 10, Loss: 0.093688, Train Acc: 96.05%\nEpoch 20, Loss: 0.088143, Train Acc: 96.25%\nTreinamento encerrado no epoch 27 (converg\u00eancia detectada).\n</code></pre>"},{"location":"projeto/projeto1/main/#implementacao-do-teste","title":"Implementa\u00e7\u00e3o do teste:","text":"<pre><code>preds_test = model.test(X_test)\n</code></pre>"},{"location":"projeto/projeto1/main/#7-curva-de-erro-e-visualizacao","title":"7. Curva de erro e visualiza\u00e7\u00e3o","text":""},{"location":"projeto/projeto1/main/#extraindo-dados-para-avaliar-modelo","title":"Extraindo dados para avaliar modelo:","text":"<pre><code>model.evaluate(X_test, y_test, plot_confusion=True, plot_roc=True, preds=preds_test)\n</code></pre>"},{"location":"projeto/projeto1/main/#resultado","title":"Resultado:","text":"<pre><code>Accuracy: 96.01%\n\n=== Pesos e Biases do Modelo ===\n\nCamada 1:\n  Pesos W1 (shape (32, 27)):\n[[-5.90536709e-01  4.66615554e-01 -4.49108095e-01  5.21877533e-03\n   3.25350763e-01  7.17445084e-02 -1.22360259e-02  1.18032302e+00\n   3.48788226e+00 -1.01996644e-02  4.23905953e-01  2.89380884e-02\n  -1.68034617e+00 -1.03658232e-01  4.08442980e-01  7.29463962e-01\n   8.41765767e-02  1.99656636e-01 -3.52814596e-01 -6.00852790e-01\n   3.88832152e-01 -1.35740680e+00 -2.79580057e-01 -4.19785134e-01\n   2.84328537e-01 -9.54605775e-01 -3.50128783e-01]\n [ 2.11480807e-01  1.00159878e-01 -3.06430613e-01  1.81622933e-01\n  -1.74595611e-01  7.60054834e-01 -1.08294362e-01 -6.50388801e-01\n  -1.00690675e+00  1.99061008e+00  3.52784516e-01  6.80065608e-01\n   2.42878895e-01 -4.88982385e-02  4.75156172e-01  1.12191729e-01\n  -4.96634271e-01  2.57593430e-01 -3.83644338e-01 -2.87492645e-01\n   5.30848775e-02 -8.70605540e-01 -3.47258423e-01 -1.41449629e-01\n   8.23021086e-01 -1.08758530e+00 -3.75258515e-01]\n [-2.07872079e+00 -1.46026838e-01 -1.58100799e-02 -1.47463794e-01\n  -3.72018818e-02  2.47253169e-01  4.14674460e-01 -1.79369636e-01\n  -1.87667054e-01  5.79683551e-01 -3.35868181e-01 -3.13726939e-02\n  -6.65274745e-01 -4.75407953e-01 -2.27500091e-01  1.13320093e+00\n   4.41707412e-01 -3.28317828e-02 -6.19415108e-01 -6.24369083e-01\n...\n   0.25893154  0.18075803  0.69442195 -0.29597184 -0.38280426 -0.71957644\n   0.460514   -0.63309972 -0.2926815  -0.44381686]]\n  Biases b3 (shape (2,)):\n[ 0.46715727 -0.46715727]\n</code></pre>"},{"location":"projeto/projeto1/main/#graficos","title":"Gr\u00e1ficos:","text":""},{"location":"projeto/projeto1/main/#matriz-de-confusao","title":"Matriz de confus\u00e3o:","text":""},{"location":"projeto/projeto1/main/#curva-roc","title":"Curva ROC","text":""},{"location":"projeto/projeto1/main/#loss-vs-epoch-accuracy-vs-epoch","title":"Loss VS Epoch / Accuracy VS Epoch","text":"<p>Podemos observar uma clara converg\u00eancia por essa imagem ainda, platou ap\u00f3s 15 epochs aproxidamente.</p>"},{"location":"projeto/projeto1/main/#8-avaliacao-do-modelo","title":"8. Avalia\u00e7\u00e3o do modelo","text":""},{"location":"projeto/projeto1/main/#olhando-os-dados-brutos","title":"Olhando os dados brutos","text":"<ul> <li>Precision: 0.936</li> <li>Recall: 0.971</li> <li>Accuracy: 96.01%</li> </ul> <p>A matriz de confus\u00e3o mostra ainda que a taxa de falsos positivos \u00e9 maior que a taxa de falsos negativos, significando que \u00e9 menos prov\u00e1vel o modelo afirmar que um passageiro saiu satisfeito, quando estava insatisfeito, do que afirmar que ele saiu insatisfeito quando estava satisfeito, que \u00e9 uma caracter\u00edstica desejada.</p> <p>Enquanto isso, a curva ROC mostra que a taxa de verdadeiros positivos \u00e9 imensamente maior que a de falsos positivos, dando ainda maior validade ao modelo.</p> <p>Por fim, devemos comparar nosso modelo atual com uma baseline para julgarmos a sua efetividade relativa:</p>"},{"location":"projeto/projeto1/main/#regressao-logistica","title":"Regress\u00e3o log\u00edstica:","text":"<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nlogreg = LogisticRegression(max_iter=500)\nlogreg.fit(X_train, y_train)\npreds = logreg.predict(X_test)\n\nprint(\"Accuracy (LogReg):\", accuracy_score(y_test, preds))\nprint(\"AUC:\", roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1]))\n</code></pre>"},{"location":"projeto/projeto1/main/#output_1","title":"Output","text":"<pre><code>Accuracy (LogReg): 0.8717277486910995\nAUC: 0.9269720121120558\n</code></pre> <p>Nosso modelo apresenta uma acur\u00e1cia e um AUC (\u00e1rea de baixo do gr\u00e1fico ROC, capacidade de separar as classes) significativamente maiores que a regress\u00e3o log\u00edstica (96% e 0.99 respectivamente). Demonstrando assim que a complexidade extra da rede neural \u00e9 justificada, ainda, pelo fato de ambos modelos terem bons desempenhos, isso mostra que parte do sinal do nosso modelo \u00e9 linear, j\u00e1 que a regress\u00e3o linear segue em partes essa caracter\u00edstica.</p>"},{"location":"projeto/projeto1/main/#random-forest","title":"Random forest:","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\npreds = rf.predict(X_test)\n\nprint(\"Accuracy (RandomForest):\", accuracy_score(y_test, preds))\n</code></pre>"},{"location":"projeto/projeto1/main/#output_2","title":"Output","text":"<pre><code>Accuracy (RandomForest): 0.963658761934093\n</code></pre> <p>Nosso modelo apresenta uma acur\u00e1cia an\u00e1loga a do random forest, que \u00e9 um modelo consolidade e validado j\u00e1. Mostrando sua efetividade e mostrando como ele tamb\u00e9m \u00e9 bom em capturar n\u00e3o linearidades, que \u00e9 uma caracter\u00edstica chave do random florest.</p>"},{"location":"projeto/projeto1/main/#dummy-aleatorio","title":"Dummy (aleat\u00f3rio):","text":"<pre><code>from sklearn.dummy import DummyClassifier\n\ndummy = DummyClassifier(strategy=\"most_frequent\")\ndummy.fit(X_train, y_train)\nprint(\"Accuracy (Dummy):\", dummy.score(X_test, y_test))\n</code></pre>"},{"location":"projeto/projeto1/main/#output_3","title":"Output","text":"<pre><code>Accuracy (Dummy): 0.5610178626424391\n</code></pre> <p>Nosso modelo tem uma acur\u00e1cia significativamente maior do que a dummy, que \u00e9 essencialmente favorecer a classe majorit\u00e1ria (que tem mais quantidade num\u00e9rica). Mostrando assim que nosso modelo \u00e9 independente da sorte e est\u00e1 acima do baseline nulo</p>"},{"location":"projeto/projeto1/main/#funcao-de-plot-auxiliar","title":"Fun\u00e7\u00e3o de plot auxiliar:","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef remove_unused_axes(fig, axes, used):\n    \"\"\"Remove eixos n\u00e3o usados (vazios) do figure.\"\"\"\n    for j in range(used, len(axes)):\n        fig.delaxes(axes[j])\n\ndef plot_distributions(df, quantitative_cols, ordinal_cols, nominal_cols,\n                       title_suffix=\"\", force_ordinal_continuous=False,\n                       delay_log_scale=True):\n    \"\"\"\n    Plota distribui\u00e7\u00f5es. \n    *********************ATEN\u00c7\u00c3O:*********************\n    Para colunas de 'delay' aplicamos np.log1p(clip(lower=0))\n    para PRESERVAR zeros e comprimir caudas (equivalente a log(1+x)). Log(0) n\u00e3o existe, n\u00e3o podemos ignorar a maioria dos dados.\n    \"\"\"\n    sns.set_style(\"white\")\n\n    if quantitative_cols:\n        n = len(quantitative_cols)\n        n_cols = 3\n        n_rows = (n + n_cols - 1) // n_cols\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, max(1, n_rows)*4))\n        axes = axes.flatten()\n        for i, col in enumerate(quantitative_cols):\n            ax = axes[i]\n            series = pd.to_numeric(df[col], errors=\"coerce\").dropna()\n            if delay_log_scale and \"delay\" in col.lower():\n                series_plot = np.log1p(series.clip(lower=0))\n                sns.histplot(series_plot, kde=False, ax=ax, stat=\"percent\", bins=40)\n                ax.set_xlabel(f\"{col} (log1p)\")\n            else:\n                sns.histplot(series, kde=False, ax=ax, stat=\"percent\", bins=40)\n            ax.set_title(col)\n            ax.set_ylabel(\"Percent\")\n            ax.tick_params(axis=\"x\", rotation=45)\n        remove_unused_axes(fig, axes, len(quantitative_cols))\n        fig.suptitle(f\"Distribui\u00e7\u00e3o das Vari\u00e1veis Quantitativas {title_suffix}\", fontsize=16, y=1.02)\n        plt.tight_layout(); plt.show()\n\n    # --- Ordinais ---\n    if ordinal_cols:\n        n = len(ordinal_cols)\n        n_cols = 3\n        n_rows = (n + n_cols - 1) // n_cols\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, max(1, n_rows)*4))\n        axes = axes.flatten()\n        for i, col in enumerate(ordinal_cols):\n            ax = axes[i]\n            coerced = pd.to_numeric(df[col], errors=\"coerce\").dropna()\n            if force_ordinal_continuous:\n                sns.histplot(coerced, kde=False, ax=ax, stat=\"percent\", bins=40)\n            else:\n                counts = coerced.value_counts(normalize=True).sort_index() * 100\n                sns.barplot(x=counts.index.astype(str), y=counts.values, ax=ax)\n            ax.set_title(col)\n            ax.set_ylabel(\"Percent\")\n            ax.tick_params(axis=\"x\", rotation=45)\n        remove_unused_axes(fig, axes, len(ordinal_cols))\n        fig.suptitle(f\"Distribui\u00e7\u00e3o das Vari\u00e1veis Ordinais {title_suffix}\", fontsize=16, y=1.02)\n        plt.tight_layout(); plt.show()\n\n    # --- Nominais ---\n    if nominal_cols:\n        # se lista vazia ou None, pula todo o bloco (evita n_rows=0)\n        n = len(nominal_cols)\n        if n &gt; 0:\n            n_cols = 3\n            n_rows = (n + n_cols - 1) // n_cols\n            fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, max(1, n_rows)*4))\n            axes = axes.flatten()\n            used = 0\n            for i, col in enumerate(nominal_cols):\n                ax = axes[i]\n                counts = df[col].astype(str).value_counts(dropna=False)\n                labels = list(counts.index)\n                sns.barplot(x=labels, y=(counts/counts.sum()*100).values, ax=ax)\n                ax.set_title(col)\n                ax.set_ylabel(\"Percent\")\n                ax.tick_params(axis=\"x\", rotation=45)\n                used += 1\n            remove_unused_axes(fig, axes, used)\n            fig.suptitle(f\"Distribui\u00e7\u00e3o das Vari\u00e1veis Nominais {title_suffix}\", fontsize=16, y=1.02)\n            plt.tight_layout(); plt.show()\n</code></pre>"}]}